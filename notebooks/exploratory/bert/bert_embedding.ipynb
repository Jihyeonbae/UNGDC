{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2df62edc-78da-44dc-87b2-e2b0f26cd9d4",
   "metadata": {},
   "source": [
    "# Extracting Word Embeddings in BERT\n",
    "\n",
    "This script tokenizes each speech document into words, and runs BERT model. Among 13 hidden layers of BERT model output, it extracts the last layer which corresponds to word embeddings. Since there are duplicate words within one speech document, it collapses multiple words into one by avering out embedding values. \n",
    "\n",
    "- This script uses Fast Tokenizer from the \"AutoTokenizer\" package. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7669c661-d15e-4cd6-98a3-f0ede7311180",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer, AutoTokenizer\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9635e45-889c-4dcc-99ca-4c8125d4b124",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1350cf95-7762-47b1-828d-302d2dae4751",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#input is \"light.csv\" which does not include stop words. \n",
    "df = pd.read_csv('../../../data/processed/light.csv')\n",
    "# Filter\n",
    "timestamps = df.year.to_list()\n",
    "texts = df.text.to_list()\n",
    "text = texts[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31cb6e02-3be0-43da-b9b8-04e12afa3921",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ccode_iso</th>\n",
       "      <th>session</th>\n",
       "      <th>year</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>AFG</td>\n",
       "      <td>7</td>\n",
       "      <td>1952</td>\n",
       "      <td>I consider great honour privilege share opport...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 ccode_iso  session  year  \\\n",
       "0           1       AFG        7  1952   \n",
       "\n",
       "                                                text  \n",
       "0  I consider great honour privilege share opport...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c4daa34-d1c6-4849-aab3-d9ae93edd7d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "  \n",
    "# Tokenize the text\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "    \n",
    "truncate_length = len(tokenized_text) - 512 + 2  # +2 to account for [CLS] and [SEP]\n",
    "        \n",
    "# Truncate the beginning and end of the text\n",
    "truncated_text = tokenized_text[truncate_length//2 : -truncate_length//2]\n",
    "\n",
    "marked_text = [\"[CLS] \"] + truncated_text + [\" [SEP]\"]\n",
    "# Add special tokens [CLS] and [SEP]\n",
    "        \n",
    "# Convert tokens to ids\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(marked_text)\n",
    "        \n",
    "# Create attention mask\n",
    "attention_mask = [1] * len(indexed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "db044cd0-5687-4339-b7be-e59127c251ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It              100\n",
      "privilege    19,800\n",
      "express       4,745\n",
      ",             1,145\n",
      "Mr            1,871\n",
      ".             1,263\n",
      "President     2,541\n",
      ",             1,714\n",
      "con           1,353\n",
      "##gratulations  1,583\n",
      "Afghanistan   6,241\n",
      "delegation    1,958\n",
      "election      2,030\n",
      ",             1,607\n",
      "just            119\n",
      "##ly          1,284\n",
      "unanimously   2,059\n",
      "voted         3,519\n",
      "Assembly      1,362\n",
      ".             1,169\n",
      "It            6,561\n",
      "also          7,616\n",
      "privilege       117\n",
      "extend        2,218\n",
      "fellow        2,174\n",
      "representatives 16,286\n",
      "greeting      9,113\n",
      "##s           3,519\n",
      "Royal           119\n",
      "Afghan        1,130\n",
      "Government    2,157\n",
      ",               117\n",
      "well         23,614\n",
      "sincere       7,279\n",
      "##st          3,681\n",
      "wishes          117\n",
      "success      11,565\n",
      "current         117\n",
      "session      21,820\n",
      "General      13,378\n",
      "Assembly     14,819\n",
      ".            10,774\n",
      "Our           3,235\n",
      "attachment    3,844\n",
      "United        1,311\n",
      "Nations       4,309\n",
      "Charter       6,551\n",
      "principles      119\n",
      "complete      1,109\n",
      "ad            7,533\n",
      "##herence     6,469\n",
      "principles   11,703\n",
      "human         2,166\n",
      "rights        6,887\n",
      "self          2,970\n",
      "-             3,768\n",
      "determination  6,551\n",
      "peoples         117\n",
      "based         1,209\n",
      "ideological   2,760\n",
      "grounds       1,692\n",
      "also          1,954\n",
      "result        4,912\n",
      "long            119\n",
      "experience    1,188\n",
      "free          7,533\n",
      "small         6,858\n",
      "country       1,912\n",
      "controversial  4,078\n",
      "events        2,019\n",
      "modern        1,583\n",
      "history       1,372\n",
      ".             2,182\n",
      "We              117\n",
      "believe       1,911\n",
      "peace         1,472\n",
      "world         3,501\n",
      "can           7,891\n",
      "secured       1,244\n",
      "bases         3,854\n",
      ",               117\n",
      "certain       7,762\n",
      "future        1,621\n",
      "prosperity    3,844\n",
      "depends       1,311\n",
      "peace         1,359\n",
      ".            11,610\n",
      "In           19,069\n",
      "saying          119\n",
      ",             1,188\n",
      "posing        1,362\n",
      "moral         2,741\n",
      "##ists       15,194\n",
      ",             7,243\n",
      "contrary      1,314\n",
      ",             4,030\n",
      "hum           1,201\n",
      "##ility         119\n",
      "expressing    1,109\n",
      "conviction   11,619\n",
      "fellow        5,172\n",
      "Member        2,170\n",
      "States        3,141\n",
      "attached     21,876\n",
      "principles    2,379\n",
      ".             1,735\n",
      "The           1,237\n",
      "attitude     21,876\n",
      "Afghanistan   1,314\n",
      "delegation    1,432\n",
      "previous        119\n",
      "sessions      4,516\n",
      "Assembly        117\n",
      "inspired      2,726\n",
      "principles    5,172\n",
      ",               117\n",
      "will          3,142\n",
      "continue      7,649\n",
      "case         13,542\n",
      "current       1,864\n",
      "session      19,967\n",
      ".             4,232\n",
      "This          2,645\n",
      "attitude      1,362\n",
      "characterized  3,035\n",
      "kind          9,175\n",
      "opposition    4,287\n",
      "towards       1,363\n",
      "country      17,274\n",
      "group           117\n",
      "countries     5,973\n",
      ",             9,535\n",
      "idea          1,892\n",
      "different    10,680\n",
      "basic           119\n",
      "ideal         1,284\n",
      "United       15,869\n",
      "Nations       2,812\n",
      ",             1,413\n",
      "distinction   2,052\n",
      "among           117\n",
      "Member        1,443\n",
      "States       19,173\n",
      "based         1,596\n",
      "geographical  5,442\n",
      "considerations  7,698\n",
      ".             1,348\n",
      "This          4,681\n",
      "world         2,019\n",
      "scene         1,583\n",
      "tremendous      119\n",
      "evolution     1,109\n",
      "last          4,574\n",
      "twelve        4,245\n",
      "years        21,839\n",
      ".            10,296\n",
      "The           1,244\n",
      "nationalist   3,854\n",
      "movements    12,500\n",
      "African       1,141\n",
      "Asian         1,436\n",
      "continents    5,136\n",
      "natural         117\n",
      "European     22,647\n",
      "American      1,268\n",
      "continents    2,365\n",
      "last         25,946\n",
      "century       1,632\n",
      ".             3,790\n",
      "Thus          1,145\n",
      ",            12,839\n",
      "supported     8,982\n",
      "movements    13,542\n",
      ",             1,632\n",
      "basis         3,794\n",
      "objective       117\n",
      "appreciation  1,244\n",
      "fact          2,325\n",
      "sincere         119\n",
      "desire        2,695\n",
      "problems     19,967\n",
      "world         1,193\n",
      "settled      14,255\n",
      "mutual       14,867\n",
      "understanding  7,926\n",
      "good          6,951\n",
      "##will          117\n",
      ",             5,136\n",
      "violent       1,723\n",
      "reactions     1,861\n",
      "blood         2,740\n",
      "##shed          119\n",
      ".             1,284\n",
      "We            4,663\n",
      "likewise      1,244\n",
      "follow        3,854\n",
      "line          7,866\n",
      "today         1,675\n",
      ",             1,362\n",
      "without       2,645\n",
      "antagonist    1,737\n",
      "##ic          1,242\n",
      "fan          22,168\n",
      "##atic          119\n",
      "##al          1,284\n",
      "feelings      1,145\n",
      "towards       4,663\n",
      "country       1,769\n",
      ".             2,645\n",
      "The           8,277\n",
      "independence    117\n",
      "Federation    1,569\n",
      "Malaya        1,741\n",
      "admission     2,670\n",
      "United        2,645\n",
      "Nations       1,505\n",
      "constitute    1,648\n",
      "one             117\n",
      "best          2,510\n",
      "examples      2,463\n",
      ",             4,485\n",
      "justification  2,335\n",
      "right        15,146\n",
      "##ful        16,334\n",
      "aspirations   1,362\n",
      "great         2,645\n",
      "nation          119\n",
      "also          1,753\n",
      "generous     22,922\n",
      "gesture      17,277\n",
      "appreciation  7,866\n",
      "great           117\n",
      "Power         1,649\n",
      ",               117\n",
      "United        2,255\n",
      "Kingdom       8,856\n",
      ".             2,999\n",
      "Both          7,243\n",
      "sincere       1,362\n",
      "##ly            119\n",
      "con          18,101\n",
      "##gra           117\n",
      "##tu          4,663\n",
      "##lated       1,632\n",
      ",             6,534\n",
      "examples        117\n",
      "followed        195\n",
      "similar      13,003\n",
      "cases        17,115\n",
      ".             2,909\n",
      "We              118\n",
      "realize       1,615\n",
      "United        6,970\n",
      "Nations       3,878\n",
      "difficulties    117\n",
      "present       2,065\n",
      "world         1,977\n",
      "problems      1,373\n",
      "considered    3,507\n",
      "many          9,286\n",
      "perspectives  4,030\n",
      ".             1,201\n",
      "We            2,403\n",
      "also            119\n",
      "realize       1,188\n",
      "human         2,612\n",
      "problems      2,810\n",
      "complicated   2,554\n",
      ",             5,070\n",
      "national        132\n",
      "political     2,059\n",
      "economic     14,740\n",
      "problems      8,050\n",
      "play         21,634\n",
      "role          6,551\n",
      ",             4,840\n",
      "individual   11,171\n",
      "problem       4,103\n",
      "regarded      6,044\n",
      "complete      9,414\n",
      "detachment    1,242\n",
      "currents      7,866\n",
      "world           119\n",
      "problems      3,458\n",
      ".            11,769\n",
      "Not           3,121\n",
      "##with       20,279\n",
      "##standing    1,359\n",
      "difficulties  8,418\n",
      ",             7,983\n",
      "however       1,362\n",
      ",               117\n",
      "reason        3,519\n",
      "appreciate      117\n",
      "normal        4,174\n",
      "evolution     4,444\n",
      "world         6,183\n",
      ".             7,514\n",
      "Fortunately   5,502\n",
      ",             4,840\n",
      "realize      11,171\n",
      "great           119\n",
      "Organization  2,994\n",
      ",             1,451\n",
      "z             1,141\n",
      "##eal         1,366\n",
      "eminent       5,857\n",
      "Secretary     1,330\n",
      "-             1,594\n",
      "General       2,498\n",
      "devoted       1,720\n",
      "officials     2,335\n",
      ",             1,126\n",
      "gone          2,605\n",
      "forward      20,473\n",
      "along         1,891\n",
      "path          1,363\n",
      "traced          117\n",
      "twelve        3,869\n",
      "years           118\n",
      "ago           2,712\n",
      ".             1,297\n",
      "This            118\n",
      "cause         3,229\n",
      "hope          1,297\n",
      "evidence        119\n",
      "progress      1,284\n",
      ";            13,099\n",
      "believe       1,116\n",
      "devotion     19,034\n",
      "ad            8,333\n",
      "##herence     4,438\n",
      "principles    2,222\n",
      "spirit        4,989\n",
      "Charter      16,286\n",
      "shall         1,234\n",
      "gradually     1,546\n",
      "overcome      1,336\n",
      "many          1,686\n",
      "difficulties  2,030\n",
      ".             3,790\n",
      "Our           1,362\n",
      "op              119\n",
      "##ti          1,284\n",
      "##mism        4,309\n",
      "based         7,181\n",
      "consciousness  6,170\n",
      "peoples      10,261\n",
      "world         1,218\n",
      ",             5,317\n",
      "peace           117\n",
      ",            20,061\n",
      "alternative   1,149\n",
      "ultimately   21,932\n",
      "identify     27,697\n",
      "respective    2,731\n",
      "policies      1,583\n",
      "spirit          119\n",
      "Charter       1,284\n",
      ".             1,774\n",
      "Each          8,333\n",
      "every           117\n",
      "one           2,407\n",
      "us              117\n",
      "convinced     1,579\n",
      "another         117\n",
      "war           6,472\n",
      "bring         4,438\n",
      "nothing      12,363\n",
      "complete        118\n",
      "an            1,734\n",
      "##ni            119\n",
      "##hil        18,101\n",
      "##ation         117\n",
      "good          9,397\n",
      ",             1,700\n",
      "worth        27,887\n",
      "-            25,449\n",
      "beautiful     7,289\n",
      "life          4,132\n",
      "-             1,314\n",
      "perhaps       3,049\n",
      "life          1,201\n",
      ".               117\n",
      "We            2,108\n",
      "Afghan        1,160\n",
      "##s           1,362\n",
      "ambition      8,755\n",
      "preserve        119\n",
      "freedom       1,706\n",
      "try           3,689\n",
      "ensure        2,030\n",
      "prosperity    3,708\n",
      "people        1,583\n",
      "order         1,444\n",
      "may           1,619\n",
      "live          5,052\n",
      "modern        1,872\n",
      "nation        2,182\n",
      "world           117\n",
      ".             9,473\n",
      "We            3,531\n",
      "attached      4,256\n",
      "traditions      119\n",
      "spiritual     1,284\n",
      "legacy        5,958\n",
      "well          8,856\n",
      "Constitution  4,301\n",
      ",             5,052\n",
      "spontaneous   1,460\n",
      "out           1,244\n",
      "##gro         3,854\n",
      "##wth           132\n",
      "nature        8,856\n",
      "country       2,860\n",
      ".               117\n",
      "We            1,218\n",
      "trying        3,767\n",
      "preserve      1,549\n",
      ",             1,366\n",
      "ready         2,739\n",
      ",             1,884\n",
      "always          118\n",
      ",             2,805\n",
      "defend        1,362\n",
      "freedom       6,534\n",
      "integrity       119\n",
      "-             6,469\n",
      "words         6,616\n",
      ".             9,441\n",
      "Fortunately   3,433\n",
      ",             1,835\n",
      "stability     5,408\n",
      "position      2,645\n",
      "sincerity       117\n",
      "neutrality    1,632\n",
      "tested        1,353\n",
      "proved          119\n",
      "last          1,284\n",
      "fifth         1,793\n",
      "years         1,763\n",
      ",               117\n",
      "especially    2,244\n",
      "two             117\n",
      "world         7,098\n",
      "wars          1,242\n",
      ".             2,645\n",
      "To            2,904\n",
      "develop       7,624\n",
      "modern          117\n",
      "##ize         1,329\n",
      "country       1,363\n",
      "need          4,158\n",
      "support         117\n",
      "assistance    5,566\n",
      "developed     4,301\n",
      "countries     1,494\n",
      ",             2,053\n",
      "grateful        117\n",
      "receive       9,441\n",
      "aid           2,086\n",
      ".            14,255\n",
      "We            6,617\n",
      "greatly       4,567\n",
      "appreciate    2,116\n",
      "technical       119\n",
      "assistance    1,284\n",
      "received      1,774\n",
      "United        1,208\n",
      "Nations         117\n",
      ";             4,103\n",
      "appreciate    2,222\n",
      "value         2,174\n",
      ",               117\n",
      "well          7,098\n",
      "opportunity   2,645\n",
      "given         2,086\n",
      "us              117\n",
      "closer        3,142\n",
      "co            7,649\n",
      "-               117\n",
      "operation     8,362\n",
      "world         1,643\n",
      "Organization  1,874\n",
      ".             9,380\n",
      "Afghanistan  19,237\n",
      "believes      1,181\n",
      "peaceful      9,486\n",
      "settlement    6,551\n",
      "international  1,268\n",
      "differences   5,299\n",
      "problems        119\n",
      ",             1,130\n",
      "great         2,458\n",
      "small           117\n",
      ".            12,747\n",
      "We            2,645\n",
      "tried         1,362\n",
      "past          9,802\n",
      ",            14,368\n",
      "success         117\n",
      ",            10,010\n",
      "settle        3,835\n",
      "many          8,362\n",
      "problems     23,242\n",
      "direct        2,645\n",
      "negotiations    117\n",
      ",             3,084\n",
      "use           4,679\n",
      "good         14,255\n",
      "offices       6,617\n",
      ",               100\n"
     ]
    }
   ],
   "source": [
    "#This one prints out the tokenized word pieces, along with indices. \n",
    "\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b3a1c68-18f3-4be4-8e46-21987d268dfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pad sequences to max_seq_length\n",
    "if len(indexed_tokens) < 512:\n",
    "    indexed_tokens.append(0)\n",
    "    attention_mask.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e1ff13f7-82ac-416c-aa35-dfa1a7aaabe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert lists to PyTorch tensors\n",
    "tokenized_texts = []\n",
    "tokens_tensors = []\n",
    "attention_masks = []\n",
    "\n",
    "    \n",
    "tokens_tensors.append(torch.tensor(indexed_tokens))\n",
    "attention_masks.append(torch.tensor(attention_mask))\n",
    "tokenized_texts.append(tokenized_text)\n",
    "\n",
    "# Convert lists to PyTorch tensors\n",
    "tokens_tensors = torch.stack(tokens_tensors)\n",
    "attention_masks = torch.stack(attention_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "84e7635d-1071-4ab9-a9ef-dee27f894f52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the BERT model\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=tokens_tensors.view(-1, tokens_tensors.size(-1)), attention_mask=attention_masks.view(-1, attention_masks.size(-1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "ab5de7a9-fded-4160-b5d7-f7c72bddbc58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512,)\n",
      "(512, 768)\n",
      "            0         1         2         3         4         5         6  \\\n",
      "0   -0.118897 -0.518255  0.159338 -0.461482 -0.003488 -0.453042 -0.212884   \n",
      "1    0.932840 -0.000157  0.029107 -0.740883  0.241258  1.106739 -0.891506   \n",
      "2    0.091746  0.066845 -0.104953  0.101292  0.095589  0.919574  1.302369   \n",
      "3    0.404450 -0.390100 -0.416748  0.195306  0.137170  0.059420 -0.139206   \n",
      "4   -0.325989 -0.142688  0.105910 -0.326140 -0.423716  0.301251 -0.412353   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "507 -0.599033  0.895255  0.119296  0.099264 -0.163519 -1.028048  0.527986   \n",
      "508 -0.301014  0.420417 -1.077342 -0.835100  1.221513 -1.501802  0.221088   \n",
      "509 -0.305659 -0.064368  0.096934 -0.491793  0.149717  0.113267  0.376909   \n",
      "510 -0.066669  0.016273  0.371624 -0.115370 -0.437943 -0.797991 -0.567259   \n",
      "511 -0.409629 -0.746030  0.954548 -0.989581 -0.666579 -0.456214 -0.482902   \n",
      "\n",
      "            7         8         9  ...       759       760       761  \\\n",
      "0   -0.229699 -0.063944 -0.421272  ...  0.009379  0.467546 -0.957577   \n",
      "1    0.490328  1.117227 -0.907664  ... -0.008901 -0.887443 -1.487365   \n",
      "2    0.050020  0.792752  0.375517  ... -0.054603  0.118318  0.898104   \n",
      "3    0.455539 -0.557390 -1.507475  ...  0.657579 -1.313308  0.362778   \n",
      "4    0.066011  0.558039 -0.382741  ... -0.073921 -0.107841  0.055453   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "507  0.678960 -0.425734  0.182220  ... -0.054713 -0.072685  0.692875   \n",
      "508  0.087812 -0.267504  0.476440  ... -0.270113  0.604884  0.166602   \n",
      "509 -0.267158  0.351870  0.077384  ... -0.479775 -0.043985 -0.980697   \n",
      "510 -0.527095 -0.034101  0.887063  ... -0.245692 -0.002529 -0.223602   \n",
      "511 -0.474517 -0.324448 -0.554452  ...  0.148158  0.841253 -0.650847   \n",
      "\n",
      "          762       763       764       765       766       767         term  \n",
      "0    0.114305 -0.369990  0.035248  0.089144 -0.146707 -0.127492       [CLS]   \n",
      "1    0.220775  0.661015 -0.453826 -0.229056 -0.360295 -1.217242  ideological  \n",
      "2   -0.426838  0.949949 -0.481893 -0.159447  0.543372 -0.487644      grounds  \n",
      "3   -0.609179  0.292855  0.595032  0.216024 -0.429597  0.052580         also  \n",
      "4    0.638443  0.295056 -0.458622 -0.004196  0.049630 -0.236417       result  \n",
      "..        ...       ...       ...       ...       ...       ...          ...  \n",
      "507  1.046173  0.120228 -0.285078 -0.163182  0.331724  0.540690           et  \n",
      "508  0.536700 -0.295725 -0.834486 -0.619155  0.289224 -1.073852        ##her  \n",
      "509 -1.175898 -0.139213 -0.271428 -0.252872 -0.979608 -0.242147          con  \n",
      "510  0.208837 -1.170713 -0.615379  0.442125 -0.005187 -0.409576         ##ci  \n",
      "511 -0.348678 -0.864865 -0.147155  0.445907  0.226717 -1.860195        [SEP]  \n",
      "\n",
      "[512 rows x 769 columns]\n"
     ]
    }
   ],
   "source": [
    "pd_words = pd.Series(marked_text, name='term')\n",
    "print(pd_words.shape)\n",
    "\n",
    "hidden_states = outputs[2][0].squeeze().numpy()\n",
    "print(hidden_states.shape)\n",
    "\n",
    "df_outputs = pd.DataFrame(hidden_states)\n",
    "df_outputs[\"term\"] = pd_words\n",
    "\n",
    "#Each column represents each term. Dimension is 768 X 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "027d341f-f0dc-4f92-ac36-92dc4d3aa53b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_outputs_embedding = df_outputs.groupby(['term']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "360b0495-40cc-4460-8b46-bce8094e7f16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_outputs_embedding.to_csv(\"../../../output/embeddings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f34578e-0fa4-46dc-b671-2ccdbfd3ea54",
   "metadata": {},
   "source": [
    "# Post Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "49bc24ff-35a8-461f-a5a9-7ce55b6d450c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 1 (initial embeddings + 12 BERT layers)\n",
      "Number of batches: 512\n",
      "Number of tokens: 768\n",
      "Type of hidden_states: <class 'torch.Tensor'>\n",
      "Tensor shape for each layer:  torch.Size([512, 768])\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of layers:\", len(hidden_states), \"(initial embeddings + 12 BERT layers)\")\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(hidden_states[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print('Type of hidden_states:', type(hidden_states))\n",
    "\n",
    "print('Tensor shape for each layer: ', hidden_states[0].size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a7444323-ab4b-4722-9a1f-d6380003db5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 768])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = hidden_states[-1]\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=0)\n",
    "\n",
    "print(token_embeddings.shape)\n",
    "word_embedding = token_embeddings[0, :].numpy()\n",
    "\n",
    "list_token_embeddings = [token_embed.tolist() for token_embed in token_embeddings]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "384c440d-3d86-47c9-bb15-a3c85f7046af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 10 x 768\n"
     ]
    }
   ],
   "source": [
    "\n",
    "token_vecs_cat = torch.cat((token_embeddings[-1:], token_embeddings[-2:], token_embeddings[-3:], token_embeddings[-4:]), dim=0)\n",
    "\n",
    "print('Shape is: %d x %d' % (token_vecs_cat.shape[0], token_vecs_cat.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "df41f716-8e26-4a16-97ae-03a1f6c20d8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0         1         2         3         4         5         6    \\\n",
      "0 -0.409629 -0.746030  0.954548 -0.989581 -0.666579 -0.456214 -0.482902   \n",
      "1 -0.066669  0.016273  0.371624 -0.115370 -0.437943 -0.797991 -0.567259   \n",
      "2 -0.409629 -0.746030  0.954548 -0.989581 -0.666579 -0.456214 -0.482902   \n",
      "3 -0.305659 -0.064368  0.096934 -0.491793  0.149717  0.113267  0.376909   \n",
      "4 -0.066669  0.016273  0.371624 -0.115370 -0.437943 -0.797991 -0.567259   \n",
      "5 -0.409629 -0.746030  0.954548 -0.989581 -0.666579 -0.456214 -0.482902   \n",
      "6 -0.301014  0.420417 -1.077342 -0.835100  1.221513 -1.501802  0.221088   \n",
      "7 -0.305659 -0.064368  0.096934 -0.491793  0.149717  0.113267  0.376909   \n",
      "8 -0.066669  0.016273  0.371624 -0.115370 -0.437943 -0.797991 -0.567259   \n",
      "9 -0.409629 -0.746030  0.954548 -0.989581 -0.666579 -0.456214 -0.482902   \n",
      "\n",
      "        7         8         9    ...       758       759       760       761  \\\n",
      "0 -0.474517 -0.324448 -0.554452  ...  0.937598  0.148158  0.841253 -0.650847   \n",
      "1 -0.527095 -0.034101  0.887063  ... -0.047614 -0.245692 -0.002529 -0.223602   \n",
      "2 -0.474517 -0.324448 -0.554452  ...  0.937598  0.148158  0.841253 -0.650847   \n",
      "3 -0.267158  0.351870  0.077384  ...  0.130521 -0.479775 -0.043985 -0.980697   \n",
      "4 -0.527095 -0.034101  0.887063  ... -0.047614 -0.245692 -0.002529 -0.223602   \n",
      "5 -0.474517 -0.324448 -0.554452  ...  0.937598  0.148158  0.841253 -0.650847   \n",
      "6  0.087812 -0.267504  0.476440  ...  0.252432 -0.270113  0.604884  0.166602   \n",
      "7 -0.267158  0.351870  0.077384  ...  0.130521 -0.479775 -0.043985 -0.980697   \n",
      "8 -0.527095 -0.034101  0.887063  ... -0.047614 -0.245692 -0.002529 -0.223602   \n",
      "9 -0.474517 -0.324448 -0.554452  ...  0.937598  0.148158  0.841253 -0.650847   \n",
      "\n",
      "        762       763       764       765       766       767  \n",
      "0 -0.348678 -0.864865 -0.147155  0.445907  0.226717 -1.860195  \n",
      "1  0.208837 -1.170713 -0.615379  0.442125 -0.005187 -0.409576  \n",
      "2 -0.348678 -0.864865 -0.147155  0.445907  0.226717 -1.860195  \n",
      "3 -1.175898 -0.139213 -0.271428 -0.252872 -0.979608 -0.242147  \n",
      "4  0.208837 -1.170713 -0.615379  0.442125 -0.005187 -0.409576  \n",
      "5 -0.348678 -0.864865 -0.147155  0.445907  0.226717 -1.860195  \n",
      "6  0.536700 -0.295725 -0.834486 -0.619155  0.289224 -1.073852  \n",
      "7 -1.175898 -0.139213 -0.271428 -0.252872 -0.979608 -0.242147  \n",
      "8  0.208837 -1.170713 -0.615379  0.442125 -0.005187 -0.409576  \n",
      "9 -0.348678 -0.864865 -0.147155  0.445907  0.226717 -1.860195  \n",
      "\n",
      "[10 rows x 768 columns]\n",
      "7680\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Collapse the embedding matrix by averaging duplicates\n",
    "collapsed_token_vecs = torch.zeros_like(token_vecs_cat)\n",
    "unique_tokens, unique_indices = torch.unique_consecutive(token_vecs_cat, return_inverse=True)\n",
    "\n",
    "# Add a dimension to unique_indices to match the dimensions of collapsed_token_vecs\n",
    "unique_indices = unique_indices.unsqueeze(1)\n",
    "\n",
    "# Iterate over unique_tokens\n",
    "for i in range(len(unique_tokens)):\n",
    "    # Create a mask for the current unique token\n",
    "    mask = (unique_indices == i).squeeze()\n",
    "\n",
    "    # Average the corresponding vectors in the original matrix\n",
    "    collapsed_token_vecs[mask] = torch.mean(token_vecs_cat[mask], dim=0)\n",
    "\n",
    "collapsed_token_vecs_np = collapsed_token_vecs.numpy()\n",
    "\n",
    "# Create a DataFrame from the collapsed embedding matrix\n",
    "df_collapsed = pd.DataFrame(collapsed_token_vecs_np)\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "#df_collapsed.to_csv('final_embeddings.csv', index=False)\n",
    "\n",
    "print(df_collapsed.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "1491974b-1b37-42d8-9b5e-227704d59cbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity between 'problems' and 'small': 0.007380645\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "term_a = \"problems\"\n",
    "term_b = \"small\"\n",
    "\n",
    "# Check if the terms are in the tokenized text\n",
    "if term_a in tokenized_text and term_b in tokenized_text:\n",
    "    # Get the indices of the terms in the tokenized text\n",
    "    index_sovereignty = tokenized_text.index(term_a)\n",
    "    index_territory = tokenized_text.index(term_b)\n",
    "\n",
    "    # Extract the embeddings for the terms\n",
    "    embedding_sovereignty = token_embeddings[index_sovereignty, :]\n",
    "    embedding_territory = token_embeddings[index_territory, :]\n",
    "\n",
    "    # Reshape the embeddings to be 2D arrays\n",
    "    embedding_sovereignty = embedding_sovereignty.reshape(1, -1)\n",
    "    embedding_territory = embedding_territory.reshape(1, -1)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarity_score = cosine_similarity(embedding_sovereignty, embedding_territory)\n",
    "\n",
    "    print(f\"Cosine Similarity between '{term_a}' and '{term_b}':\", similarity_score[0, 0])\n",
    "else:\n",
    "    print(f\"One or both of the terms '{term_a}' and '{term_b}' not found in the tokenized text.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "dfe299b6-ab97-4900-98cd-271831ce1401",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'privilege', 'express', ',', 'Mr', '.', 'President', ',', 'con', '##gratulations', 'Afghanistan', 'delegation', 'election', ',', 'just', '##ly', 'unanimously', 'voted', 'Assembly', '.', 'It', 'also', 'privilege', 'extend', 'fellow', 'representatives', 'greeting', '##s', 'Royal', 'Afghan', 'Government', ',', 'well', 'sincere', '##st', 'wishes', 'success', 'current', 'session', 'General', 'Assembly', '.', 'Our', 'attachment', 'United', 'Nations', 'Charter', 'principles', 'complete', 'ad', '##herence', 'principles', 'human', 'rights', 'self', '-', 'determination', 'peoples', 'based', 'ideological', 'grounds', 'also', 'result', 'long', 'experience', 'free', 'small', 'country', 'controversial', 'events', 'modern', 'history', '.', 'We', 'believe', 'peace', 'world', 'can', 'secured', 'bases', ',', 'certain', 'future', 'prosperity', 'depends', 'peace', '.', 'In', 'saying', ',', 'posing', 'moral', '##ists', ',', 'contrary', ',', 'hum', '##ility', 'expressing', 'conviction', 'fellow', 'Member', 'States', 'attached', 'principles', '.', 'The', 'attitude', 'Afghanistan', 'delegation', 'previous', 'sessions', 'Assembly', 'inspired', 'principles', ',', 'will', 'continue', 'case', 'current', 'session', '.', 'This', 'attitude', 'characterized', 'kind', 'opposition', 'towards', 'country', 'group', 'countries', ',', 'idea', 'different', 'basic', 'ideal', 'United', 'Nations', ',', 'distinction', 'among', 'Member', 'States', 'based', 'geographical', 'considerations', '.', 'This', 'world', 'scene', 'tremendous', 'evolution', 'last', 'twelve', 'years', '.', 'The', 'nationalist', 'movements', 'African', 'Asian', 'continents', 'natural', 'European', 'American', 'continents', 'last', 'century', '.', 'Thus', ',', 'supported', 'movements', ',', 'basis', 'objective', 'appreciation', 'fact', 'sincere', 'desire', 'problems', 'world', 'settled', 'mutual', 'understanding', 'good', '##will', ',', 'violent', 'reactions', 'blood', '##shed', '.', 'We', 'likewise', 'follow', 'line', 'today', ',', 'without', 'antagonist', '##ic', 'fan', '##atic', '##al', 'feelings', 'towards', 'country', '.', 'The', 'independence', 'Federation', 'Malaya', 'admission', 'United', 'Nations', 'constitute', 'one', 'best', 'examples', ',', 'justification', 'right', '##ful', 'aspirations', 'great', 'nation', 'also', 'generous', 'gesture', 'appreciation', 'great', 'Power', ',', 'United', 'Kingdom', '.', 'Both', 'sincere', '##ly', 'con', '##gra', '##tu', '##lated', ',', 'examples', 'followed', 'similar', 'cases', '.', 'We', 'realize', 'United', 'Nations', 'difficulties', 'present', 'world', 'problems', 'considered', 'many', 'perspectives', '.', 'We', 'also', 'realize', 'human', 'problems', 'complicated', ',', 'national', 'political', 'economic', 'problems', 'play', 'role', ',', 'individual', 'problem', 'regarded', 'complete', 'detachment', 'currents', 'world', 'problems', '.', 'Not', '##with', '##standing', 'difficulties', ',', 'however', ',', 'reason', 'appreciate', 'normal', 'evolution', 'world', '.', 'Fortunately', ',', 'realize', 'great', 'Organization', ',', 'z', '##eal', 'eminent', 'Secretary', '-', 'General', 'devoted', 'officials', ',', 'gone', 'forward', 'along', 'path', 'traced', 'twelve', 'years', 'ago', '.', 'This', 'cause', 'hope', 'evidence', 'progress', ';', 'believe', 'devotion', 'ad', '##herence', 'principles', 'spirit', 'Charter', 'shall', 'gradually', 'overcome', 'many', 'difficulties', '.', 'Our', 'op', '##ti', '##mism', 'based', 'consciousness', 'peoples', 'world', ',', 'peace', ',', 'alternative', 'ultimately', 'identify', 'respective', 'policies', 'spirit', 'Charter', '.', 'Each', 'every', 'one', 'us', 'convinced', 'another', 'war', 'bring', 'nothing', 'complete', 'an', '##ni', '##hil', '##ation', 'good', ',', 'worth', '-', 'beautiful', 'life', '-', 'perhaps', 'life', '.', 'We', 'Afghan', '##s', 'ambition', 'preserve', 'freedom', 'try', 'ensure', 'prosperity', 'people', 'order', 'may', 'live', 'modern', 'nation', 'world', '.', 'We', 'attached', 'traditions', 'spiritual', 'legacy', 'well', 'Constitution', ',', 'spontaneous', 'out', '##gro', '##wth', 'nature', 'country', '.', 'We', 'trying', 'preserve', ',', 'ready', ',', 'always', ',', 'defend', 'freedom', 'integrity', '-', 'words', '.', 'Fortunately', ',', 'stability', 'position', 'sincerity', 'neutrality', 'tested', 'proved', 'last', 'fifth', 'years', ',', 'especially', 'two', 'world', 'wars', '.', 'To', 'develop', 'modern', '##ize', 'country', 'need', 'support', 'assistance', 'developed', 'countries', ',', 'grateful', 'receive', 'aid', '.', 'We', 'greatly', 'appreciate', 'technical', 'assistance', 'received', 'United', 'Nations', ';', 'appreciate', 'value', ',', 'well', 'opportunity', 'given', 'us', 'closer', 'co', '-', 'operation', 'world', 'Organization', '.', 'Afghanistan', 'believes', 'peaceful', 'settlement', 'international', 'differences', 'problems', ',', 'great', 'small', '.', 'We', 'tried', 'past', ',', 'success', ',', 'settle', 'many', 'problems', 'direct', 'negotiations', ',', 'use', 'good', 'offices', ',', 'advice', 'technical', 'help', 'friends', ',', 'peaceful', 'means', 'con', '##ci', '##lia', '##tion', '.', 'We', 'trying', 'now', ',', 'shall', 'try', 'future', ',', 'settle', 'problems', 'means', ',', 'basis', 'objective', ',', 'un', '##p', '##re', '##ju', '##dice', '##d', 'consideration', 'principles', 'right', 'justice', '.', 'In', 'view', ',', 'regards', 'problems', 'world', 'conflicts', 'arise', ',', 'ultimate', 'reference', 'un', '##settled', 'problems', ',', 'et', '##her', 'con', '##ci', '##lia', '##tory', 'means', 'settlement', 'fail', ',', 'United', 'Nations', 'International', 'Court', 'Justice', '.', 'We', 'many', 'important', 'problems', 'agenda', 'year', 'will', 'debated', 'present', 'session', '.', 'The', 'future', 'world', 'preservation', 'international', 'peace', 'depend', '.', 'We', 'hope', ',', 'like', 'everyone', ',', 'problems', 'will', 'find', 'satisfactory', 'just', 'solution', 'spirit', 'United', 'Nations', 'Charter', '.', 'To', 'end', 'prepared', 'contribute', 'modest', 'way', 'f', '##ac', '##ilitating', 'solution', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c3a8df5d-0cee-445b-a5bb-fe8fe1696e57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2421685\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenized_texts[0].index(\"unanimously\")\n",
    "word_embedding = token_embeddings[0, word_index].numpy()\n",
    "print(word_embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
