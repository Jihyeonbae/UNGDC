{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fe4c835-b328-4fbd-ab49-c4ab11aa1cd8",
   "metadata": {},
   "source": [
    "# BERT Functions\n",
    "\n",
    "I define two functions to run Bert models. First part processes a single text document into a format that is recognizable by BERT. The second part uses the tokenized text to generate embedding values using pre-trained BERT models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6efece8a-d09f-4f83-abd1-1e2f6a9e218b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer, AutoTokenizer\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6fb1ac4-94e0-4ba4-a8c5-91bb4571f25a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9377d80a-c83f-487b-85fe-b815d77f7503",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#input is \"light.csv\" which does not include stop words. \n",
    "df = pd.read_csv('../../../data/processed/light.csv')\n",
    "# Filter\n",
    "timestamps = df.year.to_list()\n",
    "texts = df.text.to_list()\n",
    "text = texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b4bff98-9854-43ea-950d-adf7283c57b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e456560-1a36-472c-890a-d1794e814f61",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e29e9c1-71ec-49c5-a24c-b2e793a09fbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bert_preprocess(text):\n",
    "    \"\"\"\n",
    "    Preprocesses a document into a BERT-recognizable format \n",
    "    Input: text in a string format\n",
    "    output: three objects ready to be used for Bert modeling \n",
    "        marked_text (list)\n",
    "        indexed_tokens(list)\n",
    "        attention_mask(list)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Tokenize the text\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    truncate_length = len(tokenized_text) - 512 + 2  # +2 to account for [CLS] and [SEP]\n",
    "    \n",
    "    # Truncate the beginning and end of the text\n",
    "    truncated_text = tokenized_text[truncate_length//2 : -truncate_length//2]\n",
    "    \n",
    "    # Add padding\n",
    "    \n",
    "    # Add special tokens [CLS] and [SEP], convert tokens to ids, and create attention mask\n",
    "    marked_text = [\"[CLS] \"] + truncated_text + [\" [SEP]\"]\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(marked_text)\n",
    "    attention_mask = [1] * len(indexed_tokens)\n",
    "\n",
    "    # Pad sequences to max_seq_length\n",
    "    if len(indexed_tokens) < 512:\n",
    "        indexed_tokens.append(0)\n",
    "        attention_mask.append(0)\n",
    "    \n",
    "    return marked_text, indexed_tokens, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99553790-2893-4b9c-afbd-0cdab1866186",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "marked_text, indexed_tokens, attention_mask = bert_preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "44097a47-52d3-47fc-a6cf-83654f87e203",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_bert_embeddings in module __main__:\n",
      "\n",
      "get_bert_embeddings(marked_text, indexed_tokens, attention_mask)\n",
      "    input: processed text\n",
      "    output: dataframe of embedding weights for each token \n",
      "        ex) dimension of 512*768 where row represents token, column represents bert features\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(get_bert_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e08fab6d-afbd-4940-84b3-ddca619bb8d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bert_embeddings(marked_text, indexed_tokens, attention_mask):\n",
    "    \"\"\"\n",
    "    Generates embedding values for tokenized text \n",
    "    input: processed text, indexed_tokens and attention mask (all in list format)\n",
    "    output: dataframe of embedding weights for each token \n",
    "        ex) dimension of 512*768 where row represents token, column represents bert features\n",
    "    \n",
    "    \"\"\"\n",
    "    # Convert lists to PyTorch tensors\n",
    "    tokens_tensors = torch.tensor([indexed_tokens])\n",
    "    attention_masks = torch.tensor([attention_mask])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #Run the embedding\n",
    "        outputs = model(input_ids=tokens_tensors.view(-1, tokens_tensors.size(-1)), \n",
    "                        attention_mask=attention_masks.view(-1, attention_masks.size(-1)))\n",
    "\n",
    "        # Extract the hidden states \n",
    "        hidden_states = outputs[2][0].squeeze().numpy()\n",
    "        \n",
    "        # Convert to data frame\n",
    "        pd_words = pd.Series(marked_text, name='term')\n",
    "        df_outputs = pd.DataFrame(hidden_states)\n",
    "        df_outputs['term'] = pd_words\n",
    "        \n",
    "        # Move 'term' column to the first position\n",
    "        df_outputs = df_outputs[['term'] + [col for col in df_outputs.columns if col != 'term']]\n",
    "        \n",
    "        # Remove duplicate tokens by averaging them out\n",
    "        df_outputs_embedding = df_outputs.groupby(['term']).mean()\n",
    "    return df_outputs_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "474fe12f-7762-4918-8fbb-bc4761cebe5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>[SEP]</th>\n",
       "      <td>-0.409629</td>\n",
       "      <td>-0.746030</td>\n",
       "      <td>0.954548</td>\n",
       "      <td>-0.989581</td>\n",
       "      <td>-0.666579</td>\n",
       "      <td>-0.456214</td>\n",
       "      <td>-0.482902</td>\n",
       "      <td>-0.474517</td>\n",
       "      <td>-0.324448</td>\n",
       "      <td>-0.554452</td>\n",
       "      <td>...</td>\n",
       "      <td>0.937598</td>\n",
       "      <td>0.148158</td>\n",
       "      <td>0.841253</td>\n",
       "      <td>-0.650847</td>\n",
       "      <td>-0.348678</td>\n",
       "      <td>-0.864865</td>\n",
       "      <td>-0.147155</td>\n",
       "      <td>0.445907</td>\n",
       "      <td>0.226717</td>\n",
       "      <td>-1.860195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##al</th>\n",
       "      <td>-0.524131</td>\n",
       "      <td>0.479899</td>\n",
       "      <td>-0.476182</td>\n",
       "      <td>-0.390430</td>\n",
       "      <td>0.571847</td>\n",
       "      <td>0.344540</td>\n",
       "      <td>-0.890469</td>\n",
       "      <td>-0.778739</td>\n",
       "      <td>-0.369768</td>\n",
       "      <td>-0.369769</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.796855</td>\n",
       "      <td>-0.407300</td>\n",
       "      <td>0.098990</td>\n",
       "      <td>0.707968</td>\n",
       "      <td>-0.098804</td>\n",
       "      <td>-1.176002</td>\n",
       "      <td>0.376283</td>\n",
       "      <td>0.099251</td>\n",
       "      <td>-0.782967</td>\n",
       "      <td>-0.062336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##atic</th>\n",
       "      <td>-1.258919</td>\n",
       "      <td>-1.749650</td>\n",
       "      <td>0.189675</td>\n",
       "      <td>-1.009726</td>\n",
       "      <td>1.250488</td>\n",
       "      <td>-0.127454</td>\n",
       "      <td>-0.174475</td>\n",
       "      <td>0.132727</td>\n",
       "      <td>0.210920</td>\n",
       "      <td>0.159041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.256487</td>\n",
       "      <td>0.588991</td>\n",
       "      <td>-1.045582</td>\n",
       "      <td>-0.885318</td>\n",
       "      <td>-0.134264</td>\n",
       "      <td>-0.281723</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.318711</td>\n",
       "      <td>-0.002958</td>\n",
       "      <td>0.664755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##ation</th>\n",
       "      <td>-0.321686</td>\n",
       "      <td>-0.599758</td>\n",
       "      <td>0.114046</td>\n",
       "      <td>-0.144932</td>\n",
       "      <td>0.953009</td>\n",
       "      <td>-0.936072</td>\n",
       "      <td>0.038542</td>\n",
       "      <td>0.732640</td>\n",
       "      <td>0.667231</td>\n",
       "      <td>-0.173093</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.485267</td>\n",
       "      <td>0.513310</td>\n",
       "      <td>0.546876</td>\n",
       "      <td>0.632246</td>\n",
       "      <td>0.262685</td>\n",
       "      <td>-0.174507</td>\n",
       "      <td>0.236400</td>\n",
       "      <td>-0.632015</td>\n",
       "      <td>-0.797872</td>\n",
       "      <td>-1.417162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##ci</th>\n",
       "      <td>-0.209563</td>\n",
       "      <td>0.008983</td>\n",
       "      <td>0.226166</td>\n",
       "      <td>-0.037539</td>\n",
       "      <td>-0.431536</td>\n",
       "      <td>-0.854313</td>\n",
       "      <td>-0.930581</td>\n",
       "      <td>-0.289056</td>\n",
       "      <td>0.014480</td>\n",
       "      <td>0.836836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035582</td>\n",
       "      <td>-0.291024</td>\n",
       "      <td>0.075818</td>\n",
       "      <td>0.033696</td>\n",
       "      <td>0.396918</td>\n",
       "      <td>-0.968211</td>\n",
       "      <td>-0.381200</td>\n",
       "      <td>0.396398</td>\n",
       "      <td>0.112123</td>\n",
       "      <td>-0.490416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>words</th>\n",
       "      <td>0.397322</td>\n",
       "      <td>-0.091103</td>\n",
       "      <td>0.350278</td>\n",
       "      <td>0.339480</td>\n",
       "      <td>0.188769</td>\n",
       "      <td>-0.122880</td>\n",
       "      <td>0.119570</td>\n",
       "      <td>-0.366882</td>\n",
       "      <td>0.528809</td>\n",
       "      <td>0.175150</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.813381</td>\n",
       "      <td>-0.366494</td>\n",
       "      <td>0.197109</td>\n",
       "      <td>-0.413089</td>\n",
       "      <td>0.482895</td>\n",
       "      <td>-0.190066</td>\n",
       "      <td>-0.568232</td>\n",
       "      <td>0.127709</td>\n",
       "      <td>-0.095941</td>\n",
       "      <td>0.127358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>world</th>\n",
       "      <td>0.991384</td>\n",
       "      <td>-0.232762</td>\n",
       "      <td>0.670588</td>\n",
       "      <td>0.508774</td>\n",
       "      <td>0.526652</td>\n",
       "      <td>0.077158</td>\n",
       "      <td>-0.581935</td>\n",
       "      <td>0.377466</td>\n",
       "      <td>0.073109</td>\n",
       "      <td>0.119594</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.505561</td>\n",
       "      <td>0.136024</td>\n",
       "      <td>0.009123</td>\n",
       "      <td>0.495411</td>\n",
       "      <td>0.342043</td>\n",
       "      <td>-1.043479</td>\n",
       "      <td>-0.131786</td>\n",
       "      <td>-0.333381</td>\n",
       "      <td>-0.390346</td>\n",
       "      <td>-0.141330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worth</th>\n",
       "      <td>-0.107531</td>\n",
       "      <td>0.469970</td>\n",
       "      <td>-0.674297</td>\n",
       "      <td>0.848895</td>\n",
       "      <td>0.115055</td>\n",
       "      <td>0.818750</td>\n",
       "      <td>-0.049695</td>\n",
       "      <td>-0.560952</td>\n",
       "      <td>-0.509267</td>\n",
       "      <td>-0.288416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032292</td>\n",
       "      <td>-0.336956</td>\n",
       "      <td>0.691349</td>\n",
       "      <td>0.267413</td>\n",
       "      <td>-0.045436</td>\n",
       "      <td>0.660112</td>\n",
       "      <td>1.184719</td>\n",
       "      <td>-0.789721</td>\n",
       "      <td>0.254513</td>\n",
       "      <td>-0.662614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>years</th>\n",
       "      <td>0.478471</td>\n",
       "      <td>-0.254025</td>\n",
       "      <td>-0.260787</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>-0.097176</td>\n",
       "      <td>0.219434</td>\n",
       "      <td>-0.531882</td>\n",
       "      <td>-0.517204</td>\n",
       "      <td>0.729554</td>\n",
       "      <td>0.117572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.398866</td>\n",
       "      <td>0.446946</td>\n",
       "      <td>-0.115180</td>\n",
       "      <td>-0.216491</td>\n",
       "      <td>-0.296925</td>\n",
       "      <td>0.555254</td>\n",
       "      <td>-0.139911</td>\n",
       "      <td>-0.880707</td>\n",
       "      <td>-0.042803</td>\n",
       "      <td>0.468518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z</th>\n",
       "      <td>0.215062</td>\n",
       "      <td>-0.684214</td>\n",
       "      <td>-0.219748</td>\n",
       "      <td>-0.830821</td>\n",
       "      <td>0.585531</td>\n",
       "      <td>-0.721144</td>\n",
       "      <td>-0.274569</td>\n",
       "      <td>0.207120</td>\n",
       "      <td>0.271554</td>\n",
       "      <td>-1.113561</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.068485</td>\n",
       "      <td>-0.185102</td>\n",
       "      <td>-0.254614</td>\n",
       "      <td>-0.406646</td>\n",
       "      <td>-0.101535</td>\n",
       "      <td>-0.219950</td>\n",
       "      <td>-0.111580</td>\n",
       "      <td>-0.428931</td>\n",
       "      <td>-0.333112</td>\n",
       "      <td>-0.023003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>321 rows × 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6    \\\n",
       "term                                                                            \n",
       " [SEP]  -0.409629 -0.746030  0.954548 -0.989581 -0.666579 -0.456214 -0.482902   \n",
       "##al    -0.524131  0.479899 -0.476182 -0.390430  0.571847  0.344540 -0.890469   \n",
       "##atic  -1.258919 -1.749650  0.189675 -1.009726  1.250488 -0.127454 -0.174475   \n",
       "##ation -0.321686 -0.599758  0.114046 -0.144932  0.953009 -0.936072  0.038542   \n",
       "##ci    -0.209563  0.008983  0.226166 -0.037539 -0.431536 -0.854313 -0.930581   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "words    0.397322 -0.091103  0.350278  0.339480  0.188769 -0.122880  0.119570   \n",
       "world    0.991384 -0.232762  0.670588  0.508774  0.526652  0.077158 -0.581935   \n",
       "worth   -0.107531  0.469970 -0.674297  0.848895  0.115055  0.818750 -0.049695   \n",
       "years    0.478471 -0.254025 -0.260787  0.681818 -0.097176  0.219434 -0.531882   \n",
       "z        0.215062 -0.684214 -0.219748 -0.830821  0.585531 -0.721144 -0.274569   \n",
       "\n",
       "              7         8         9    ...       758       759       760  \\\n",
       "term                                   ...                                 \n",
       " [SEP]  -0.474517 -0.324448 -0.554452  ...  0.937598  0.148158  0.841253   \n",
       "##al    -0.778739 -0.369768 -0.369769  ... -0.796855 -0.407300  0.098990   \n",
       "##atic   0.132727  0.210920  0.159041  ...  0.256487  0.588991 -1.045582   \n",
       "##ation  0.732640  0.667231 -0.173093  ... -0.485267  0.513310  0.546876   \n",
       "##ci    -0.289056  0.014480  0.836836  ...  0.035582 -0.291024  0.075818   \n",
       "...           ...       ...       ...  ...       ...       ...       ...   \n",
       "words   -0.366882  0.528809  0.175150  ... -0.813381 -0.366494  0.197109   \n",
       "world    0.377466  0.073109  0.119594  ... -0.505561  0.136024  0.009123   \n",
       "worth   -0.560952 -0.509267 -0.288416  ...  0.032292 -0.336956  0.691349   \n",
       "years   -0.517204  0.729554  0.117572  ...  0.398866  0.446946 -0.115180   \n",
       "z        0.207120  0.271554 -1.113561  ... -0.068485 -0.185102 -0.254614   \n",
       "\n",
       "              761       762       763       764       765       766       767  \n",
       "term                                                                           \n",
       " [SEP]  -0.650847 -0.348678 -0.864865 -0.147155  0.445907  0.226717 -1.860195  \n",
       "##al     0.707968 -0.098804 -1.176002  0.376283  0.099251 -0.782967 -0.062336  \n",
       "##atic  -0.885318 -0.134264 -0.281723  0.578229 -0.318711 -0.002958  0.664755  \n",
       "##ation  0.632246  0.262685 -0.174507  0.236400 -0.632015 -0.797872 -1.417162  \n",
       "##ci     0.033696  0.396918 -0.968211 -0.381200  0.396398  0.112123 -0.490416  \n",
       "...           ...       ...       ...       ...       ...       ...       ...  \n",
       "words   -0.413089  0.482895 -0.190066 -0.568232  0.127709 -0.095941  0.127358  \n",
       "world    0.495411  0.342043 -1.043479 -0.131786 -0.333381 -0.390346 -0.141330  \n",
       "worth    0.267413 -0.045436  0.660112  1.184719 -0.789721  0.254513 -0.662614  \n",
       "years   -0.216491 -0.296925  0.555254 -0.139911 -0.880707 -0.042803  0.468518  \n",
       "z       -0.406646 -0.101535 -0.219950 -0.111580 -0.428931 -0.333112 -0.023003  \n",
       "\n",
       "[321 rows x 768 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bert_embeddings(marked_text, indexed_tokens, attention_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
