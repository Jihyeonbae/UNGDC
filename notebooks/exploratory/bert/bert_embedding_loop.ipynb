{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7199bdb9-9892-4d46-a057-676103c7f4db",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Word embeddings generated by BERT Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe4bd6f-2a30-4c94-86e1-eaaea50f39b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, AutoTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Initialize BERT model and tokenizer\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "# Read the input DataFrame\n",
    "df = pd.read_csv('../../../data/processed/light.csv')\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "output_folder = \"../../../output/bert_embeddings\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Extract relevant information from the row\n",
    "    ccode_iso = row['ccode_iso']\n",
    "    year = row['year']\n",
    "    text = row['text']\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Truncate or pad text to fit within max_seq_length\n",
    "    truncate_length = len(tokenized_text) - 512 + 2  # +2 to account for [CLS] and [SEP]\n",
    "    truncated_text = tokenized_text[truncate_length//2 : -truncate_length//2]\n",
    "\n",
    "    # Add special tokens [CLS] and [SEP], convert tokens to ids, and create attention mask\n",
    "    marked_text = [\"[CLS] \"] + truncated_text + [\" [SEP]\"]\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(marked_text)\n",
    "    attention_mask = [1] * len(indexed_tokens)\n",
    "\n",
    "    # Pad sequences to max_seq_length\n",
    "    if len(indexed_tokens) < 512:\n",
    "        indexed_tokens.append(0)\n",
    "        attention_mask.append(0)\n",
    "\n",
    "    # Convert lists to PyTorch tensors\n",
    "    tokens_tensors = torch.tensor([indexed_tokens])\n",
    "    attention_masks = torch.tensor([attention_mask])\n",
    "\n",
    "    # Run the BERT model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=tokens_tensors.view(-1, tokens_tensors.size(-1)), attention_mask=attention_masks.view(-1, attention_masks.size(-1)))\n",
    "\n",
    "    # Extract the hidden states and create a DataFrame\n",
    "    hidden_states = outputs[2][0].squeeze().numpy()\n",
    "    pd_words = pd.Series(marked_text, name='term')\n",
    "    df_outputs = pd.DataFrame(hidden_states)\n",
    "    df_outputs['term'] = pd_words\n",
    "\n",
    "    # Move 'term' column to the first position\n",
    "    df_outputs = df_outputs[['term'] + [col for col in df_outputs.columns if col != 'term']]\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    output_file = os.path.join(output_folder, f'embedding_{ccode_iso}_{year}.csv')\n",
    "    df_outputs.to_csv(output_file, index=False)\n",
    "\n",
    "# Record the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the total time taken\n",
    "total_time = end_time - start_time\n",
    "print(f'Total time taken: {total_time} seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
