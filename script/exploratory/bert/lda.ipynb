{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7837319b-7f91-4862-a8d5-8e4012c8c3f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LDA for UNGDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03b3dbad-c02f-461c-8186-d15bcdbd91a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import spacy \n",
    "from spacy import displacy\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel, CoherenceModel, LsiModel, HdpModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3450b79f-e7c2-4996-a8f8-1844f9166908",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the input DataFrame\n",
    "df = pd.read_csv('../../../data/processed/cleaned.csv')\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "output_folder = \"../../../output/LDA_python\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "040111b2-f5e7-48df-b8ca-195fa59bec0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "timestamps = df.year.to_list()\n",
    "texts = df.text.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4135cd8-4e4b-4e14-983b-4ff697a8e31f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "197abd50-2cc3-4aec-bbe5-5a1cc4455d9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jihyeonbae/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['will', 'must'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48684f7d-7aa5-429e-ae17-08b2398c75f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jihyeonbae/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['consider', 'great', 'honour', 'privilege', 'share', 'opportunity', 'united', 'nation', 'momentous', 'occasion', 'world', 'organization', 'embodies', 'hope', 'aspiration', 'people', 'world', 'peace', 'prosperity', 'prospect', 'better', 'fruitful', 'life', 'task', 'reaffirm', 'help', 'realize', 'aim', 'purpose', 'expressed']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenize text into words\n",
    "    words = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize words\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return words\n",
    "\n",
    "# Apply preprocessing to each sentence\n",
    "data_words = [preprocess_text(sentence) for sentence in data]\n",
    "\n",
    "# Print the first 30 words of the preprocessed text\n",
    "print(data_words[:1][0][:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfb48824-c7a4-48b3-bb94-d11593ea21bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "# Create Corpus\n",
    "texts = data_words\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1aea711-ed74-4c30-909e-e81b4048a39e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics for 1946-1951 time window:\n",
      "[(0,\n",
      "  '0.001*\"would\" + 0.001*\"could\" + 0.000*\"cannot\" + 0.000*\"veto\" + '\n",
      "  '0.000*\"greece\" + 0.000*\"powers\" + 0.000*\"korea\" + 0.000*\"ussr\" + '\n",
      "  '0.000*\"chinese\" + 0.000*\"delegation\"'),\n",
      " (1,\n",
      "  '0.000*\"would\" + 0.000*\"could\" + 0.000*\"cannot\" + 0.000*\"siam\" + '\n",
      "  '0.000*\"veto\" + 0.000*\"polish\" + 0.000*\"powers\" + 0.000*\"ass\" + '\n",
      "  '0.000*\"arabs\" + 0.000*\"lebanese\"'),\n",
      " (2,\n",
      "  '0.001*\"would\" + 0.001*\"could\" + 0.001*\"cannot\" + 0.000*\"veto\" + '\n",
      "  '0.000*\"india\" + 0.000*\"powers\" + 0.000*\"aggression\" + 0.000*\"might\" + '\n",
      "  '0.000*\"jews\" + 0.000*\"charter\"'),\n",
      " (3,\n",
      "  '0.001*\"would\" + 0.000*\"could\" + 0.000*\"cannot\" + 0.000*\"netherlands\" + '\n",
      "  '0.000*\"korea\" + 0.000*\"ussr\" + 0.000*\"veto\" + 0.000*\"delegation\" + '\n",
      "  '0.000*\"speech\" + 0.000*\"aggression\"'),\n",
      " (4,\n",
      "  '0.001*\"would\" + 0.001*\"could\" + 0.000*\"cannot\" + 0.000*\"greece\" + '\n",
      "  '0.000*\"greek\" + 0.000*\"veto\" + 0.000*\"ussr\" + 0.000*\"aggression\" + '\n",
      "  '0.000*\"czechoslovakia\" + 0.000*\"american\"'),\n",
      " (5,\n",
      "  '0.002*\"would\" + 0.001*\"could\" + 0.000*\"cannot\" + 0.000*\"veto\" + '\n",
      "  '0.000*\"ussr\" + 0.000*\"powers\" + 0.000*\"australian\" + 0.000*\"article\" + '\n",
      "  '0.000*\"delegation\" + 0.000*\"korea\"'),\n",
      " (6,\n",
      "  '0.001*\"would\" + 0.001*\"could\" + 0.000*\"cannot\" + 0.000*\"polish\" + '\n",
      "  '0.000*\"veto\" + 0.000*\"aggression\" + 0.000*\"article\" + 0.000*\"ussr\" + '\n",
      "  '0.000*\"korea\" + 0.000*\"unanimity\"'),\n",
      " (7,\n",
      "  '0.001*\"would\" + 0.000*\"could\" + 0.000*\"veto\" + 0.000*\"cannot\" + '\n",
      "  '0.000*\"hyderabad\" + 0.000*\"italy\" + 0.000*\"ussr\" + 0.000*\"powers\" + '\n",
      "  '0.000*\"greece\" + 0.000*\"might\"'),\n",
      " (8,\n",
      "  '0.001*\"would\" + 0.001*\"cannot\" + 0.000*\"could\" + 0.000*\"veto\" + '\n",
      "  '0.000*\"korea\" + 0.000*\"ussr\" + 0.000*\"aggression\" + 0.000*\"ought\" + '\n",
      "  '0.000*\"powers\" + 0.000*\"delegation\"'),\n",
      " (9,\n",
      "  '0.002*\"would\" + 0.001*\"could\" + 0.000*\"cannot\" + 0.000*\"veto\" + '\n",
      "  '0.000*\"ussr\" + 0.000*\"league\" + 0.000*\"korea\" + 0.000*\"privilege\" + '\n",
      "  '0.000*\"might\" + 0.000*\"france\"')]\n",
      "\n",
      "\n",
      "Topics for 1950-1955 time window:\n",
      "[(0,\n",
      "  '0.001*\"would\" + 0.001*\"cannot\" + 0.000*\"could\" + 0.000*\"armaments\" + '\n",
      "  '0.000*\"atomic\" + 0.000*\"ussr\" + 0.000*\"korea\" + 0.000*\"france\" + '\n",
      "  '0.000*\"aggression\" + 0.000*\"korean\"'),\n",
      " (1,\n",
      "  '0.001*\"would\" + 0.001*\"cannot\" + 0.000*\"could\" + 0.000*\"korea\" + '\n",
      "  '0.000*\"ussr\" + 0.000*\"aggression\" + 0.000*\"propaganda\" + 0.000*\"bolivia\" + '\n",
      "  '0.000*\"aggressive\" + 0.000*\"defence\"'),\n",
      " (2,\n",
      "  '0.001*\"would\" + 0.000*\"cannot\" + 0.000*\"could\" + 0.000*\"korea\" + '\n",
      "  '0.000*\"veto\" + 0.000*\"communist\" + 0.000*\"aggression\" + 0.000*\"egypt\" + '\n",
      "  '0.000*\"aggressive\" + 0.000*\"powers\"'),\n",
      " (3,\n",
      "  '0.001*\"would\" + 0.000*\"cannot\" + 0.000*\"could\" + 0.000*\"korea\" + '\n",
      "  '0.000*\"ussr\" + 0.000*\"yugoslav\" + 0.000*\"aggression\" + 0.000*\"yugoslavia\" + '\n",
      "  '0.000*\"questions\" + 0.000*\"delegation\"'),\n",
      " (4,\n",
      "  '0.001*\"would\" + 0.000*\"cannot\" + 0.000*\"could\" + 0.000*\"korea\" + '\n",
      "  '0.000*\"france\" + 0.000*\"aggression\" + 0.000*\"netherlands\" + 0.000*\"tunisia\" '\n",
      "  '+ 0.000*\"morocco\" + 0.000*\"charter\"'),\n",
      " (5,\n",
      "  '0.001*\"would\" + 0.000*\"cannot\" + 0.000*\"could\" + 0.000*\"korea\" + '\n",
      "  '0.000*\"delegation\" + 0.000*\"poland\" + 0.000*\"atomic\" + 0.000*\"ussr\" + '\n",
      "  '0.000*\"connexion\" + 0.000*\"greece\"'),\n",
      " (6,\n",
      "  '0.001*\"would\" + 0.001*\"cannot\" + 0.000*\"could\" + 0.000*\"communist\" + '\n",
      "  '0.000*\"korea\" + 0.000*\"czechoslovak\" + 0.000*\"germany\" + 0.000*\"powers\" + '\n",
      "  '0.000*\"armistice\" + 0.000*\"delegation\"'),\n",
      " (7,\n",
      "  '0.002*\"would\" + 0.001*\"cannot\" + 0.001*\"could\" + 0.000*\"korea\" + '\n",
      "  '0.000*\"armistice\" + 0.000*\"ussr\" + 0.000*\"atomic\" + 0.000*\"korean\" + '\n",
      "  '0.000*\"communist\" + 0.000*\"aggression\"'),\n",
      " (8,\n",
      "  '0.002*\"would\" + 0.001*\"cannot\" + 0.001*\"could\" + 0.000*\"korea\" + '\n",
      "  '0.000*\"communist\" + 0.000*\"aggression\" + 0.000*\"ussr\" + 0.000*\"france\" + '\n",
      "  '0.000*\"armistice\" + 0.000*\"atomic\"'),\n",
      " (9,\n",
      "  '0.001*\"would\" + 0.001*\"cannot\" + 0.000*\"could\" + 0.000*\"korea\" + '\n",
      "  '0.000*\"thailand\" + 0.000*\"aggression\" + 0.000*\"ought\" + 0.000*\"charter\" + '\n",
      "  '0.000*\"delegation\" + 0.000*\"italy\"')]\n",
      "\n",
      "\n",
      "Topics for 1954-1959 time window:\n",
      "[(0,\n",
      "  '0.001*\"would\" + 0.000*\"cannot\" + 0.000*\"could\" + 0.000*\"czechoslovak\" + '\n",
      "  '0.000*\"algeria\" + 0.000*\"ought\" + 0.000*\"disarmament\" + 0.000*\"france\" + '\n",
      "  '0.000*\"french\" + 0.000*\"ministries\"'),\n",
      " (1,\n",
      "  '0.002*\"would\" + 0.001*\"cannot\" + 0.001*\"could\" + 0.000*\"france\" + '\n",
      "  '0.000*\"disarmament\" + 0.000*\"atomic\" + 0.000*\"algerian\" + '\n",
      "  '0.000*\"delegation\" + 0.000*\"powers\" + 0.000*\"soviet\"'),\n",
      " (2,\n",
      "  '0.002*\"would\" + 0.001*\"cannot\" + 0.001*\"could\" + 0.000*\"hungarian\" + '\n",
      "  '0.000*\"hungary\" + 0.000*\"german\" + 0.000*\"france\" + 0.000*\"disarmament\" + '\n",
      "  '0.000*\"algerian\" + 0.000*\"algeria\"'),\n",
      " (3,\n",
      "  '0.001*\"would\" + 0.001*\"cannot\" + 0.000*\"could\" + 0.000*\"hungary\" + '\n",
      "  '0.000*\"austrian\" + 0.000*\"disarmament\" + 0.000*\"paraguay\" + 0.000*\"atomic\" '\n",
      "  '+ 0.000*\"panama\" + 0.000*\"austria\"'),\n",
      " (4,\n",
      "  '0.002*\"would\" + 0.001*\"cannot\" + 0.001*\"could\" + 0.000*\"algerian\" + '\n",
      "  '0.000*\"egypt\" + 0.000*\"france\" + 0.000*\"powers\" + 0.000*\"hungarian\" + '\n",
      "  '0.000*\"disarmament\" + 0.000*\"algeria\"'),\n",
      " (5,\n",
      "  '0.001*\"would\" + 0.001*\"cannot\" + 0.000*\"could\" + 0.000*\"algeria\" + '\n",
      "  '0.000*\"panama\" + 0.000*\"arab\" + 0.000*\"disarmament\" + 0.000*\"france\" + '\n",
      "  '0.000*\"algerian\" + 0.000*\"powers\"'),\n",
      " (6,\n",
      "  '0.001*\"would\" + 0.001*\"cannot\" + 0.000*\"could\" + 0.000*\"poland\" + '\n",
      "  '0.000*\"polish\" + 0.000*\"egypt\" + 0.000*\"france\" + 0.000*\"hungary\" + '\n",
      "  '0.000*\"algerians\" + 0.000*\"delegation\"'),\n",
      " (7,\n",
      "  '0.002*\"would\" + 0.001*\"cannot\" + 0.001*\"could\" + 0.000*\"egypt\" + '\n",
      "  '0.000*\"hungary\" + 0.000*\"israel\" + 0.000*\"france\" + 0.000*\"arab\" + '\n",
      "  '0.000*\"geneva\" + 0.000*\"powers\"'),\n",
      " (8,\n",
      "  '0.003*\"would\" + 0.001*\"cannot\" + 0.001*\"could\" + 0.000*\"delegation\" + '\n",
      "  '0.000*\"laos\" + 0.000*\"hungary\" + 0.000*\"disarmament\" + 0.000*\"powers\" + '\n",
      "  '0.000*\"kingdom\" + 0.000*\"france\"'),\n",
      " (9,\n",
      "  '0.001*\"would\" + 0.001*\"cannot\" + 0.000*\"could\" + 0.000*\"atomic\" + '\n",
      "  '0.000*\"ghana\" + 0.000*\"malaya\" + 0.000*\"czechoslovak\" + 0.000*\"israel\" + '\n",
      "  '0.000*\"disarmament\" + 0.000*\"arab\"')]\n",
      "\n",
      "\n",
      "Topics for 1958-1963 time window:\n",
      "[(0,\n",
      "  '0.001*\"would\" + 0.001*\"cannot\" + 0.001*\"could\" + 0.000*\"congo\" + '\n",
      "  '0.000*\"algerian\" + 0.000*\"colonial\" + 0.000*\"disarmament\" + 0.000*\"tests\" + '\n",
      "  '0.000*\"algeria\" + 0.000*\"powers\"'),\n",
      " (1,\n",
      "  '0.002*\"would\" + 0.001*\"cannot\" + 0.001*\"could\" + 0.000*\"disarmament\" + '\n",
      "  '0.000*\"algerian\" + 0.000*\"algeria\" + 0.000*\"congo\" + 0.000*\"tests\" + '\n",
      "  '0.000*\"powers\" + 0.000*\"delegation\"'),\n",
      " (2,\n",
      "  '0.003*\"would\" + 0.002*\"cannot\" + 0.001*\"could\" + 0.000*\"algerian\" + '\n",
      "  '0.000*\"algeria\" + 0.000*\"disarmament\" + 0.000*\"congo\" + 0.000*\"berlin\" + '\n",
      "  '0.000*\"germany\" + 0.000*\"tests\"'),\n",
      " (3,\n",
      "  '0.003*\"would\" + 0.001*\"cannot\" + 0.001*\"could\" + 0.000*\"disarmament\" + '\n",
      "  '0.000*\"powers\" + 0.000*\"congo\" + 0.000*\"tests\" + 0.000*\"colonial\" + '\n",
      "  '0.000*\"algeria\" + 0.000*\"nuclear\"'),\n",
      " (4,\n",
      "  '0.001*\"would\" + 0.001*\"cannot\" + 0.000*\"could\" + 0.000*\"algeria\" + '\n",
      "  '0.000*\"congo\" + 0.000*\"ghana\" + 0.000*\"arab\" + 0.000*\"tests\" + '\n",
      "  '0.000*\"powers\" + 0.000*\"disarmament\"'),\n",
      " (5,\n",
      "  '0.002*\"would\" + 0.001*\"cannot\" + 0.001*\"could\" + 0.000*\"disarmament\" + '\n",
      "  '0.000*\"powers\" + 0.000*\"communist\" + 0.000*\"tests\" + 0.000*\"congo\" + '\n",
      "  '0.000*\"colonial\" + 0.000*\"delegation\"'),\n",
      " (6,\n",
      "  '0.003*\"would\" + 0.001*\"cannot\" + 0.001*\"could\" + 0.000*\"disarmament\" + '\n",
      "  '0.000*\"powers\" + 0.000*\"congo\" + 0.000*\"algerian\" + 0.000*\"tests\" + '\n",
      "  '0.000*\"arab\" + 0.000*\"man\"'),\n",
      " (7,\n",
      "  '0.001*\"would\" + 0.001*\"cannot\" + 0.000*\"could\" + 0.000*\"disarmament\" + '\n",
      "  '0.000*\"powers\" + 0.000*\"nuclear\" + 0.000*\"colonial\" + 0.000*\"algerian\" + '\n",
      "  '0.000*\"congo\" + 0.000*\"spain\"'),\n",
      " (8,\n",
      "  '0.002*\"would\" + 0.001*\"cannot\" + 0.001*\"could\" + 0.000*\"congo\" + '\n",
      "  '0.000*\"algerian\" + 0.000*\"disarmament\" + 0.000*\"algeria\" + '\n",
      "  '0.000*\"netherlands\" + 0.000*\"congolese\" + 0.000*\"powers\"'),\n",
      " (9,\n",
      "  '0.002*\"would\" + 0.001*\"cannot\" + 0.001*\"could\" + 0.000*\"congo\" + '\n",
      "  '0.000*\"disarmament\" + 0.000*\"tests\" + 0.000*\"powers\" + 0.000*\"algerian\" + '\n",
      "  '0.000*\"colonial\" + 0.000*\"nuclear\"')]\n",
      "\n",
      "\n",
      "Topics for 1962-1967 time window:\n",
      "[(0,\n",
      "  '0.002*\"would\" + 0.001*\"cannot\" + 0.001*\"could\" + 0.000*\"powers\" + '\n",
      "  '0.000*\"colonial\" + 0.000*\"congo\" + 0.000*\"china\" + 0.000*\"disarmament\" + '\n",
      "  '0.000*\"nuclear\" + 0.000*\"communist\"'),\n",
      " (1,\n",
      "  '0.002*\"would\" + 0.002*\"cannot\" + 0.001*\"could\" + 0.000*\"africa\" + '\n",
      "  '0.000*\"powers\" + 0.000*\"south\" + 0.000*\"delegation\" + 0.000*\"disarmament\" + '\n",
      "  '0.000*\"nuclear\" + 0.000*\"portugal\"'),\n",
      " (2,\n",
      "  '0.001*\"would\" + 0.001*\"cannot\" + 0.000*\"could\" + 0.000*\"nepal\" + '\n",
      "  '0.000*\"africa\" + 0.000*\"colonial\" + 0.000*\"african\" + 0.000*\"powers\" + '\n",
      "  '0.000*\"congo\" + 0.000*\"delegation\"'),\n",
      " (3,\n",
      "  '0.002*\"would\" + 0.001*\"cannot\" + 0.001*\"could\" + 0.000*\"nuclear\" + '\n",
      "  '0.000*\"powers\" + 0.000*\"disarmament\" + 0.000*\"delegation\" + 0.000*\"china\" + '\n",
      "  '0.000*\"congo\" + 0.000*\"colonial\"'),\n",
      " (4,\n",
      "  '0.001*\"would\" + 0.001*\"cannot\" + 0.001*\"could\" + 0.000*\"laos\" + '\n",
      "  '0.000*\"italian\" + 0.000*\"pakistan\" + 0.000*\"delegation\" + '\n",
      "  '0.000*\"colonialism\" + 0.000*\"arab\" + 0.000*\"kashmir\"'),\n",
      " (5,\n",
      "  '0.004*\"would\" + 0.002*\"cannot\" + 0.001*\"could\" + 0.000*\"nuclear\" + '\n",
      "  '0.000*\"powers\" + 0.000*\"china\" + 0.000*\"delegation\" + 0.000*\"africa\" + '\n",
      "  '0.000*\"disarmament\" + 0.000*\"african\"'),\n",
      " (6,\n",
      "  '0.002*\"would\" + 0.001*\"cannot\" + 0.001*\"could\" + 0.000*\"nuclear\" + '\n",
      "  '0.000*\"china\" + 0.000*\"powers\" + 0.000*\"delegation\" + 0.000*\"disarmament\" + '\n",
      "  '0.000*\"committee\" + 0.000*\"south\"'),\n",
      " (7,\n",
      "  '0.002*\"would\" + 0.001*\"cannot\" + 0.001*\"could\" + 0.000*\"powers\" + '\n",
      "  '0.000*\"colonial\" + 0.000*\"disarmament\" + 0.000*\"nuclear\" + 0.000*\"congo\" + '\n",
      "  '0.000*\"colonialism\" + 0.000*\"somali\"'),\n",
      " (8,\n",
      "  '0.004*\"would\" + 0.002*\"cannot\" + 0.001*\"could\" + 0.000*\"nuclear\" + '\n",
      "  '0.000*\"delegation\" + 0.000*\"colonial\" + 0.000*\"powers\" + '\n",
      "  '0.000*\"disarmament\" + 0.000*\"china\" + 0.000*\"africa\"'),\n",
      " (9,\n",
      "  '0.003*\"would\" + 0.002*\"cannot\" + 0.001*\"could\" + 0.000*\"nuclear\" + '\n",
      "  '0.000*\"colonial\" + 0.000*\"china\" + 0.000*\"disarmament\" + 0.000*\"delegation\" '\n",
      "  '+ 0.000*\"venezuela\" + 0.000*\"powers\"')]\n",
      "\n",
      "\n",
      "Topics for 1966-1971 time window:\n",
      "[(0,\n",
      "  '0.002*\"would\" + 0.001*\"cannot\" + 0.001*\"could\" + 0.000*\"china\" + '\n",
      "  '0.000*\"arab\" + 0.000*\"aggression\" + 0.000*\"powers\" + 0.000*\"israel\" + '\n",
      "  '0.000*\"delegation\" + 0.000*\"colonial\"'),\n",
      " (1,\n",
      "  '0.001*\"would\" + 0.001*\"cannot\" + 0.000*\"could\" + 0.000*\"arab\" + '\n",
      "  '0.000*\"israel\" + 0.000*\"china\" + 0.000*\"powers\" + 0.000*\"aggression\" + '\n",
      "  '0.000*\"socialist\" + 0.000*\"thant\"'),\n",
      " (2,\n",
      "  '0.001*\"would\" + 0.001*\"cannot\" + 0.000*\"could\" + 0.000*\"africa\" + '\n",
      "  '0.000*\"rhodesia\" + 0.000*\"portugal\" + 0.000*\"china\" + 0.000*\"delegation\" + '\n",
      "  '0.000*\"gabon\" + 0.000*\"south\"'),\n",
      " (3,\n",
      "  '0.002*\"would\" + 0.002*\"cannot\" + 0.001*\"could\" + 0.000*\"israel\" + '\n",
      "  '0.000*\"arab\" + 0.000*\"decade\" + 0.000*\"china\" + 0.000*\"powers\" + '\n",
      "  '0.000*\"africa\" + 0.000*\"south\"'),\n",
      " (4,\n",
      "  '0.003*\"would\" + 0.002*\"cannot\" + 0.001*\"could\" + 0.000*\"arab\" + '\n",
      "  '0.000*\"israel\" + 0.000*\"powers\" + 0.000*\"aggression\" + 0.000*\"china\" + '\n",
      "  '0.000*\"nuclear\" + 0.000*\"delegation\"'),\n",
      " (5,\n",
      "  '0.002*\"would\" + 0.001*\"cannot\" + 0.001*\"could\" + 0.000*\"israel\" + '\n",
      "  '0.000*\"powers\" + 0.000*\"delegation\" + 0.000*\"south\" + 0.000*\"colonial\" + '\n",
      "  '0.000*\"arab\" + 0.000*\"aggression\"'),\n",
      " (6,\n",
      "  '0.002*\"would\" + 0.002*\"cannot\" + 0.001*\"could\" + 0.000*\"china\" + '\n",
      "  '0.000*\"powers\" + 0.000*\"delegation\" + 0.000*\"israel\" + 0.000*\"portugal\" + '\n",
      "  '0.000*\"aggression\" + 0.000*\"south\"'),\n",
      " (7,\n",
      "  '0.002*\"would\" + 0.001*\"cannot\" + 0.001*\"could\" + 0.000*\"powers\" + '\n",
      "  '0.000*\"arab\" + 0.000*\"israel\" + 0.000*\"china\" + 0.000*\"delegation\" + '\n",
      "  '0.000*\"aggression\" + 0.000*\"rhodesia\"'),\n",
      " (8,\n",
      "  '0.002*\"would\" + 0.002*\"cannot\" + 0.001*\"could\" + 0.000*\"china\" + '\n",
      "  '0.000*\"czechoslovakia\" + 0.000*\"czechoslovak\" + 0.000*\"south\" + '\n",
      "  '0.000*\"ecuador\" + 0.000*\"africa\" + 0.000*\"nuclear\"'),\n",
      " (9,\n",
      "  '0.001*\"would\" + 0.001*\"cannot\" + 0.001*\"could\" + 0.000*\"arab\" + '\n",
      "  '0.000*\"china\" + 0.000*\"colonial\" + 0.000*\"israel\" + 0.000*\"socialist\" + '\n",
      "  '0.000*\"powers\" + 0.000*\"struggle\"')]\n",
      "\n",
      "\n",
      "Topics for 1970-1975 time window:\n",
      "[(0,\n",
      "  '0.001*\"would\" + 0.001*\"cannot\" + 0.001*\"could\" + 0.000*\"detente\" + '\n",
      "  '0.000*\"delegation\" + 0.000*\"powers\" + 0.000*\"dialog\" + 0.000*\"struggle\" + '\n",
      "  '0.000*\"arab\" + 0.000*\"imperialism\"'),\n",
      " (1,\n",
      "  '0.001*\"would\" + 0.001*\"cannot\" + 0.000*\"could\" + 0.000*\"israel\" + '\n",
      "  '0.000*\"arab\" + 0.000*\"dialog\" + 0.000*\"detente\" + 0.000*\"honduras\" + '\n",
      "  '0.000*\"territories\" + 0.000*\"israeli\"'),\n",
      " (2,\n",
      "  '0.002*\"would\" + 0.001*\"cannot\" + 0.001*\"could\" + 0.000*\"detente\" + '\n",
      "  '0.000*\"cyprus\" + 0.000*\"portugal\" + 0.000*\"israel\" + 0.000*\"delegation\" + '\n",
      "  '0.000*\"powers\" + 0.000*\"colonial\"'),\n",
      " (3,\n",
      "  '0.002*\"would\" + 0.001*\"cannot\" + 0.001*\"could\" + 0.000*\"arab\" + '\n",
      "  '0.000*\"israel\" + 0.000*\"portugal\" + 0.000*\"portuguese\" + 0.000*\"powers\" + '\n",
      "  '0.000*\"africa\" + 0.000*\"dialog\"'),\n",
      " (4,\n",
      "  '0.004*\"would\" + 0.002*\"cannot\" + 0.001*\"could\" + 0.001*\"detente\" + '\n",
      "  '0.000*\"portugal\" + 0.000*\"powers\" + 0.000*\"delegation\" + 0.000*\"europe\" + '\n",
      "  '0.000*\"portuguese\" + 0.000*\"arab\"'),\n",
      " (5,\n",
      "  '0.001*\"would\" + 0.001*\"cannot\" + 0.001*\"could\" + 0.000*\"detente\" + '\n",
      "  '0.000*\"arab\" + 0.000*\"chile\" + 0.000*\"israel\" + 0.000*\"colonialism\" + '\n",
      "  '0.000*\"powers\" + 0.000*\"struggle\"'),\n",
      " (6,\n",
      "  '0.002*\"would\" + 0.002*\"cannot\" + 0.001*\"could\" + 0.000*\"detente\" + '\n",
      "  '0.000*\"cyprus\" + 0.000*\"delegation\" + 0.000*\"dialog\" + 0.000*\"europe\" + '\n",
      "  '0.000*\"powers\" + 0.000*\"colonialism\"'),\n",
      " (7,\n",
      "  '0.002*\"cannot\" + 0.001*\"would\" + 0.001*\"could\" + 0.000*\"detente\" + '\n",
      "  '0.000*\"khmer\" + 0.000*\"bangladesh\" + 0.000*\"cambodia\" + 0.000*\"delegation\" '\n",
      "  '+ 0.000*\"portuguese\" + 0.000*\"portugal\"'),\n",
      " (8,\n",
      "  '0.001*\"would\" + 0.001*\"cannot\" + 0.000*\"could\" + 0.000*\"detente\" + '\n",
      "  '0.000*\"portuguese\" + 0.000*\"portugal\" + 0.000*\"colonial\" + 0.000*\"dialog\" + '\n",
      "  '0.000*\"delegation\" + 0.000*\"sea\"'),\n",
      " (9,\n",
      "  '0.002*\"would\" + 0.002*\"cannot\" + 0.001*\"could\" + 0.000*\"detente\" + '\n",
      "  '0.000*\"portugal\" + 0.000*\"israel\" + 0.000*\"portuguese\" + 0.000*\"europe\" + '\n",
      "  '0.000*\"dialog\" + 0.000*\"program\"')]\n",
      "\n",
      "\n",
      "Topics for 1974-1979 time window:\n",
      "[(0,\n",
      "  '0.002*\"would\" + 0.001*\"cannot\" + 0.001*\"could\" + 0.000*\"co\" + '\n",
      "  '0.000*\"operation\" + 0.000*\"namibia\" + 0.000*\"detente\" + 0.000*\"zimbabwe\" + '\n",
      "  '0.000*\"struggle\" + 0.000*\"cyprus\"'),\n",
      " (1,\n",
      "  '0.003*\"would\" + 0.002*\"cannot\" + 0.001*\"could\" + 0.001*\"co\" + '\n",
      "  '0.001*\"operation\" + 0.000*\"cyprus\" + 0.000*\"detente\" + 0.000*\"zimbabwe\" + '\n",
      "  '0.000*\"africa\" + 0.000*\"arab\"'),\n",
      " (2,\n",
      "  '0.002*\"would\" + 0.001*\"cannot\" + 0.001*\"could\" + 0.000*\"co\" + '\n",
      "  '0.000*\"operation\" + 0.000*\"detente\" + 0.000*\"zimbabwe\" + 0.000*\"namibia\" + '\n",
      "  '0.000*\"disarmament\" + 0.000*\"australia\"'),\n",
      " (3,\n",
      "  '0.003*\"would\" + 0.001*\"cannot\" + 0.001*\"could\" + 0.000*\"co\" + '\n",
      "  '0.000*\"detente\" + 0.000*\"zimbabwe\" + 0.000*\"operation\" + 0.000*\"namibia\" + '\n",
      "  '0.000*\"cyprus\" + 0.000*\"struggle\"'),\n",
      " (4,\n",
      "  '0.002*\"would\" + 0.001*\"cannot\" + 0.001*\"could\" + 0.000*\"co\" + '\n",
      "  '0.000*\"operation\" + 0.000*\"zimbabwe\" + 0.000*\"struggle\" + '\n",
      "  '0.000*\"independence\" + 0.000*\"arab\" + 0.000*\"namibia\"'),\n",
      " (5,\n",
      "  '0.002*\"would\" + 0.002*\"cannot\" + 0.001*\"could\" + 0.000*\"co\" + '\n",
      "  '0.000*\"operation\" + 0.000*\"detente\" + 0.000*\"zimbabwe\" + 0.000*\"namibia\" + '\n",
      "  '0.000*\"africa\" + 0.000*\"south\"'),\n",
      " (6,\n",
      "  '0.003*\"would\" + 0.002*\"cannot\" + 0.001*\"could\" + 0.001*\"co\" + '\n",
      "  '0.000*\"operation\" + 0.000*\"detente\" + 0.000*\"arab\" + 0.000*\"cyprus\" + '\n",
      "  '0.000*\"africa\" + 0.000*\"zimbabwe\"'),\n",
      " (7,\n",
      "  '0.002*\"would\" + 0.001*\"cannot\" + 0.001*\"could\" + 0.000*\"co\" + '\n",
      "  '0.000*\"operation\" + 0.000*\"bolivia\" + 0.000*\"detente\" + 0.000*\"israel\" + '\n",
      "  '0.000*\"namibia\" + 0.000*\"struggle\"'),\n",
      " (8,\n",
      "  '0.002*\"would\" + 0.001*\"cannot\" + 0.001*\"could\" + 0.000*\"co\" + '\n",
      "  '0.000*\"operation\" + 0.000*\"detente\" + 0.000*\"zimbabwe\" + 0.000*\"namibia\" + '\n",
      "  '0.000*\"struggle\" + 0.000*\"cyprus\"'),\n",
      " (9,\n",
      "  '0.003*\"would\" + 0.002*\"cannot\" + 0.001*\"could\" + 0.001*\"cyprus\" + '\n",
      "  '0.001*\"co\" + 0.000*\"operation\" + 0.000*\"zimbabwe\" + 0.000*\"detente\" + '\n",
      "  '0.000*\"namibia\" + 0.000*\"africa\"')]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/jihyeonbae/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jihyeonbae/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m window_data_words_bigrams \u001b[38;5;241m=\u001b[39m [bigram_phraser[doc] \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m window_data_words]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Remove stopwords\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m window_data_words_filtered \u001b[38;5;241m=\u001b[39m remove_stopwords(window_data_words_bigrams)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Generate TF-IDF weighted representations\u001b[39;00m\n\u001b[1;32m     26\u001b[0m window_corpus \u001b[38;5;241m=\u001b[39m [id2word\u001b[38;5;241m.\u001b[39mdoc2bow(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m window_data_words_filtered]\n",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m, in \u001b[0;36mremove_stopwords\u001b[0;34m(texts)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_stopwords\u001b[39m(texts):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [[word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m simple_preprocess(\u001b[38;5;28mstr\u001b[39m(doc)) \n\u001b[1;32m      7\u001b[0m              \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words] \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m texts]\n",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_stopwords\u001b[39m(texts):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [[word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m simple_preprocess(\u001b[38;5;28mstr\u001b[39m(doc)) \n\u001b[1;32m      7\u001b[0m              \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words] \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m texts]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/utils.py:310\u001b[0m, in \u001b[0;36msimple_preprocess\u001b[0;34m(doc, deacc, min_len, max_len)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimple_preprocess\u001b[39m(doc, deacc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, min_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m):\n\u001b[1;32m    289\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert a document into a list of lowercase tokens, ignoring tokens that are too short or too long.\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    Uses :func:`~gensim.utils.tokenize` internally.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    308\u001b[0m \n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    311\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokenize(doc, lower\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, deacc\u001b[38;5;241m=\u001b[39mdeacc, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m min_len \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(token) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m max_len \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m token\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    313\u001b[0m     ]\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/utils.py:310\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimple_preprocess\u001b[39m(doc, deacc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, min_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m):\n\u001b[1;32m    289\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert a document into a list of lowercase tokens, ignoring tokens that are too short or too long.\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    Uses :func:`~gensim.utils.tokenize` internally.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    308\u001b[0m \n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    311\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokenize(doc, lower\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, deacc\u001b[38;5;241m=\u001b[39mdeacc, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m min_len \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(token) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m max_len \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m token\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    313\u001b[0m     ]\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/gensim/utils.py:285\u001b[0m, in \u001b[0;36msimple_tokenize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Tokenize input test using :const:`gensim.utils.PAT_ALPHABETIC`.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    282\u001b[0m \n\u001b[1;32m    283\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m PAT_ALPHABETIC\u001b[38;5;241m.\u001b[39mfinditer(text):\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m match\u001b[38;5;241m.\u001b[39mgroup()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim import corpora, models\n",
    "\n",
    "# Define TF-IDF model\n",
    "tfidf_model = models.TfidfModel(dictionary=id2word)\n",
    "\n",
    "window_size = 5  # Define the size of the time window (e.g., 5 years)\n",
    "step_size = 4    # Define the step size for moving the window (e.g., 1 year)\n",
    "\n",
    "for start_year in range(min(timestamps), max(timestamps) - window_size + 1, step_size):\n",
    "    end_year = start_year + window_size\n",
    "    \n",
    "    # Filter texts within the current time window\n",
    "    window_texts = df[df['year'].between(start_year, end_year)]['text'].tolist()\n",
    "    \n",
    "    # Tokenize the texts into words and generate bigrams\n",
    "    window_data_words = list(sent_to_words(window_texts))\n",
    "    bigram = Phrases(window_data_words, min_count=5, threshold=100)\n",
    "    bigram_phraser = Phraser(bigram)\n",
    "    window_data_words_bigrams = [bigram_phraser[doc] for doc in window_data_words]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    window_data_words_filtered = remove_stopwords(window_data_words_bigrams)\n",
    "    \n",
    "    # Generate TF-IDF weighted representations\n",
    "    window_corpus = [id2word.doc2bow(text) for text in window_data_words_filtered]\n",
    "    window_tfidf_corpus = tfidf_model[window_corpus]\n",
    "    \n",
    "    # Train LDA model using TF-IDF weighted representations\n",
    "    window_lda_model = gensim.models.LdaMulticore(corpus=window_tfidf_corpus,\n",
    "                                                   id2word=id2word,\n",
    "                                                   num_topics=num_topics)\n",
    "    \n",
    "    # Print or store the topics for the current time window\n",
    "    print(f\"Topics for {start_year}-{end_year} time window:\")\n",
    "    pprint(window_lda_model.print_topics())\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf02285-426d-4ac8-a685-a48139779429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate the heatmap for a pair of models\n",
    "\n",
    "generate_heatmap <- function(model1, model2, correlation_threshold = 0.9) {\n",
    "  phi1 <- model1$phi\n",
    "  phi2 <- model2$phi\n",
    "\n",
    "  phi1_df <- as.data.frame(phi1)\n",
    "  phi2_df <- as.data.frame(phi2)\n",
    "\n",
    "  order_phi1 <- order(colMeans(phi1_df), decreasing = TRUE)\n",
    "  order_phi2 <- order(colMeans(phi2_df), decreasing = TRUE)\n",
    "\n",
    "  phi1_df <- phi1_df[, order_phi1]\n",
    "  phi2_df <- phi2_df[, order_phi2]\n",
    "\n",
    "  all_terms <- union(colnames(phi1_df), colnames(phi2_df))\n",
    "\n",
    "  phi1_union <- dplyr::bind_cols(phi1_df, setNames(data.frame(matrix(0, nrow = nrow(phi1_df), ncol = length(setdiff(all_terms, colnames(phi1_df))))), setdiff(all_terms, colnames(phi1_df))))\n",
    "  \n",
    "  phi2_union <- dplyr::bind_cols(phi2_df, setNames(data.frame(matrix(0, nrow = nrow(phi2_df), ncol = length(setdiff(all_terms, colnames(phi2_df))))), setdiff(all_terms, colnames(phi2_df))))\n",
    "\n",
    "  phi1_union <- phi1_union[, order(colnames(phi1_union))]\n",
    "  phi2_union <- phi2_union[, order(colnames(phi2_union))]\n",
    "\n",
    "  dim(phi1_union)\n",
    "  dim(phi2_union)\n",
    "\n",
    "  cor_matrix <- cor(t(phi1_union), t(phi2_union))\n",
    "\n",
    "\n",
    "  # Heatmap for correlation matrix\n",
    "  heatmap.2(cor_matrix,\n",
    "            Rowv = FALSE, Colv = FALSE,\n",
    "            col = heat.colors(16),\n",
    "            trace = \"none\", # no row/column names\n",
    "            key = TRUE, keysize = 1.5,\n",
    "            density.info = \"none\", margins = c(5, 5),\n",
    "            cexCol = 1, cexRow = 1, # adjust text size\n",
    "            notecol = \"black\", notecex = 0.7,\n",
    "            xlab = \"Time 2\",\n",
    "            ylab = \"Time 1\",\n",
    "            symkey = FALSE)\n",
    "\n",
    "  return(list(phi1_union = phi1_union, phi2_union = phi2_union, cor_matrix = cor_matrix))\n",
    "}\n",
    "```\n",
    "\n",
    "## Rows with high correlation\n",
    "\n",
    "```{r}\n",
    "# Function to print the ordered rows for each topic with high correlation\n",
    "print_ordered_rows <- function(phi1_union, phi2_union, cor_matrix, high_corr_indices, correlation_threshold = 0.9) {\n",
    "  # Find indices where correlation is higher than the threshold\n",
    "  high_corr_indices <- which(cor_matrix > correlation_threshold & !is.na(cor_matrix), arr.ind = TRUE)\n",
    "\n",
    "  # Create an empty list to store results\n",
    "  result_list <- list()\n",
    "\n",
    "  # Print the ordered rows for each topic with high correlation\n",
    "  for (i in seq_len(nrow(high_corr_indices))) {\n",
    "    model1_topic <- high_corr_indices[i, 1]\n",
    "    model2_topic <- high_corr_indices[i, 2]\n",
    "\n",
    "    # Print the ordered rows for each model's topic\n",
    "    cat(paste(\"Model 1 - Topic\", model1_topic), \"\\n\")\n",
    "    phi1_result_row <- orderBasedOnRow(phi1_union, model1_topic)\n",
    "\n",
    "    cat(paste(\"Model 2 - Topic\", model2_topic), \"\\n\")\n",
    "    phi2_result_row <- orderBasedOnRow(phi2_union, model2_topic)\n",
    "\n",
    "    # Convert result rows to long format\n",
    "    phi1_result_long <- phi1_result_row %>%\n",
    "      tidyr::pivot_longer(everything(), names_to = \"term_1\", values_to = \"probability_1\")\n",
    "\n",
    "    phi2_result_long <- phi2_result_row %>%\n",
    "      tidyr::pivot_longer(everything(), names_to = \"term_2\", values_to = \"probability_2\")\n",
    "\n",
    "    # Combine phi1 and phi2 results\n",
    "    pair <- knitr::kable(bind_cols(phi1_result_long, phi2_result_long))\n",
    "\n",
    "    # Append the result to the list\n",
    "    result_list[[i]] <- pair\n",
    "  }\n",
    "\n",
    "  # Combine all results into a single dataframe\n",
    "  final_result <- do.call(bind_rows, result_list)\n",
    "\n",
    "  return(final_result)\n",
    "}\n",
    "```\n",
    "\n",
    "## Execute functions over pairs\n",
    "\n",
    "```{r}\n",
    "# Loop through pairs of models to generate heatmaps and print results\n",
    "for (i in 1:(length(lda_models) - 1)) {\n",
    "  model1 <- lda_models[[i]]\n",
    "  model2 <- lda_models[[i + 1]]\n",
    "\n",
    "  result <- generate_heatmap(model1, model2, correlation_threshold = 0.6)\n",
    "  \n",
    "  phi1_union <- result$phi1_union\n",
    "  phi2_union <- result$phi2_union\n",
    "  cor_matrix <- result$cor_matrix\n",
    "\n",
    "  # Print ordered rows only if there are high correlations\n",
    "  if (any(cor_matrix > 0.6, na.rm = TRUE)) {\n",
    "    phi1_result <- phi1_union[, order(colMeans(phi1_union), decreasing = TRUE)]\n",
    "    phi2_result <- phi2_union[, order(colMeans(phi2_union), decreasing = TRUE)]\n",
    "\n",
    "    # Call the modified function and pass high_corr_indices as an argument\n",
    "    final_result <- print_ordered_rows(phi1_result, phi2_result, cor_matrix, high_corr_indices)\n",
    "    print(final_result)\n",
    "  }\n",
    "}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
