{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2df62edc-78da-44dc-87b2-e2b0f26cd9d4",
   "metadata": {},
   "source": [
    "# Extracting Word Embeddings in BERT\n",
    "\n",
    "This script tokenizes each speech document into words, and runs BERT model. Among 13 hidden layers of BERT model output, it extracts the last layer which corresponds to word embeddings. Since there are duplicate words within one speech document, it collapses multiple words into one by averaging out embedding values. \n",
    "\n",
    "- This script uses Fast Tokenizer from the \"AutoTokenizer\" package. \n",
    "- This script is for a *single* document and generates cosine similarity between words within a single document. \n",
    "\n",
    "## BERT (Bidirectional Encoder Representations from Transformers)\n",
    "BERT model [Devlin et al., 2019](https://arxiv.org/pdf/1810.04805.pdf) is a masked language modeling architecture by conditioning word vectors on both left and right side of the word's context. Masked language model randomly masks (or blurs) tokens, and is trained to predict the missing token. \n",
    "\n",
    "**Contextual(or dynamic) embeddings** (including BERT) are different from GloVe (static embedding) in that single word is assigned with more than one vector representations, depending on the context. Even the same word \"apple\" will be represented by different vectors, allowing us to distinguish iPhone from fruit. Then encoders learn and predict the masked token (*not word*) by using the entire set of tokens in the given input - entire speech in this context. \n",
    "\n",
    "BERT, one of the dynamic embeddings model, is structured with below features. \n",
    "- subword tokens: less common words are split into multiple subwords tokens. \n",
    "- 13 hidden layers\n",
    "- each hidden layer has a size of 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7669c661-d15e-4cd6-98a3-f0ede7311180",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer, AutoTokenizer\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9635e45-889c-4dcc-99ca-4c8125d4b124",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "1350cf95-7762-47b1-828d-302d2dae4751",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#input is \"light.csv\" which does not include stop words. \n",
    "df = pd.read_csv('../../../data/processed/light.csv')\n",
    "meta = pd.read_csv('../../../data/processed/meta.csv')\n",
    "# Filter\n",
    "timestamps = df.year.to_list()\n",
    "texts = df.text.to_list()\n",
    "\n",
    "# I subtract first document delivered by Afghanistan in 1952. \n",
    "text = texts[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "db04117f-8c05-467d-bc12-28cd2a5cf1f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>year</th>\n",
       "      <th>ccode</th>\n",
       "      <th>gwcode</th>\n",
       "      <th>mid_dispute</th>\n",
       "      <th>mid_num_dispute</th>\n",
       "      <th>cow_num_inter</th>\n",
       "      <th>cow_inter</th>\n",
       "      <th>cow_num_civil</th>\n",
       "      <th>cow_civil</th>\n",
       "      <th>...</th>\n",
       "      <th>v2xcl_acjst</th>\n",
       "      <th>v2xcs_ccsi</th>\n",
       "      <th>v2x_freexp</th>\n",
       "      <th>v2xme_altinf</th>\n",
       "      <th>v2smgovdom</th>\n",
       "      <th>v2smgovfilcap</th>\n",
       "      <th>v2smgovfilprc</th>\n",
       "      <th>v2smgovshutcap</th>\n",
       "      <th>v2smgovshut</th>\n",
       "      <th>v2xedvd_me_cent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10568.000000</td>\n",
       "      <td>10568.000000</td>\n",
       "      <td>10217.000000</td>\n",
       "      <td>9197.00000</td>\n",
       "      <td>9464.000000</td>\n",
       "      <td>9464.000000</td>\n",
       "      <td>4567.000000</td>\n",
       "      <td>4567.000000</td>\n",
       "      <td>4567.000000</td>\n",
       "      <td>4567.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>9520.000000</td>\n",
       "      <td>9520.00000</td>\n",
       "      <td>9520.000000</td>\n",
       "      <td>9520.000000</td>\n",
       "      <td>3858.000000</td>\n",
       "      <td>3858.000000</td>\n",
       "      <td>3858.000000</td>\n",
       "      <td>3858.000000</td>\n",
       "      <td>3858.000000</td>\n",
       "      <td>9352.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>47.198808</td>\n",
       "      <td>1992.198808</td>\n",
       "      <td>453.044142</td>\n",
       "      <td>451.58356</td>\n",
       "      <td>0.607777</td>\n",
       "      <td>0.607777</td>\n",
       "      <td>0.019926</td>\n",
       "      <td>0.041822</td>\n",
       "      <td>0.147580</td>\n",
       "      <td>0.147580</td>\n",
       "      <td>...</td>\n",
       "      <td>0.583364</td>\n",
       "      <td>0.57373</td>\n",
       "      <td>0.564268</td>\n",
       "      <td>0.553715</td>\n",
       "      <td>0.101634</td>\n",
       "      <td>-0.086477</td>\n",
       "      <td>0.469499</td>\n",
       "      <td>-0.163223</td>\n",
       "      <td>0.602092</td>\n",
       "      <td>0.465796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>19.695417</td>\n",
       "      <td>19.695417</td>\n",
       "      <td>261.311267</td>\n",
       "      <td>248.60928</td>\n",
       "      <td>1.545809</td>\n",
       "      <td>1.545809</td>\n",
       "      <td>0.139760</td>\n",
       "      <td>0.200204</td>\n",
       "      <td>0.354722</td>\n",
       "      <td>0.354722</td>\n",
       "      <td>...</td>\n",
       "      <td>0.291367</td>\n",
       "      <td>0.31867</td>\n",
       "      <td>0.322586</td>\n",
       "      <td>0.330446</td>\n",
       "      <td>1.378806</td>\n",
       "      <td>1.268826</td>\n",
       "      <td>1.543879</td>\n",
       "      <td>1.286865</td>\n",
       "      <td>1.291013</td>\n",
       "      <td>0.322204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1946.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.00800</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>-3.640000</td>\n",
       "      <td>-3.329000</td>\n",
       "      <td>-3.898000</td>\n",
       "      <td>-3.162000</td>\n",
       "      <td>-4.169000</td>\n",
       "      <td>0.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>32.000000</td>\n",
       "      <td>1977.000000</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>230.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.318000</td>\n",
       "      <td>0.27375</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.216000</td>\n",
       "      <td>-0.920500</td>\n",
       "      <td>-1.042750</td>\n",
       "      <td>-0.666000</td>\n",
       "      <td>-1.160250</td>\n",
       "      <td>-0.376000</td>\n",
       "      <td>0.161000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>1995.000000</td>\n",
       "      <td>450.000000</td>\n",
       "      <td>451.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.622000</td>\n",
       "      <td>0.63700</td>\n",
       "      <td>0.618000</td>\n",
       "      <td>0.679000</td>\n",
       "      <td>-0.022000</td>\n",
       "      <td>-0.070000</td>\n",
       "      <td>0.866000</td>\n",
       "      <td>-0.215000</td>\n",
       "      <td>1.013000</td>\n",
       "      <td>0.423000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>64.000000</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>663.000000</td>\n",
       "      <td>652.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.88500</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>0.853000</td>\n",
       "      <td>1.153750</td>\n",
       "      <td>0.961500</td>\n",
       "      <td>1.823000</td>\n",
       "      <td>0.918000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.794000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>77.000000</td>\n",
       "      <td>2022.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>950.00000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.997000</td>\n",
       "      <td>0.98300</td>\n",
       "      <td>0.992000</td>\n",
       "      <td>0.977000</td>\n",
       "      <td>2.877000</td>\n",
       "      <td>2.943000</td>\n",
       "      <td>2.601000</td>\n",
       "      <td>2.507000</td>\n",
       "      <td>2.004000</td>\n",
       "      <td>0.981000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            session          year         ccode      gwcode  mid_dispute  \\\n",
       "count  10568.000000  10568.000000  10217.000000  9197.00000  9464.000000   \n",
       "mean      47.198808   1992.198808    453.044142   451.58356     0.607777   \n",
       "std       19.695417     19.695417    261.311267   248.60928     1.545809   \n",
       "min        1.000000   1946.000000      2.000000     2.00000     0.000000   \n",
       "25%       32.000000   1977.000000    220.000000   230.00000     0.000000   \n",
       "50%       50.000000   1995.000000    450.000000   451.00000     0.000000   \n",
       "75%       64.000000   2009.000000    663.000000   652.00000     1.000000   \n",
       "max       77.000000   2022.000000    990.000000   950.00000    34.000000   \n",
       "\n",
       "       mid_num_dispute  cow_num_inter    cow_inter  cow_num_civil  \\\n",
       "count      9464.000000    4567.000000  4567.000000    4567.000000   \n",
       "mean          0.607777       0.019926     0.041822       0.147580   \n",
       "std           1.545809       0.139760     0.200204       0.354722   \n",
       "min           0.000000       0.000000     0.000000       0.000000   \n",
       "25%           0.000000       0.000000     0.000000       0.000000   \n",
       "50%           0.000000       0.000000     0.000000       0.000000   \n",
       "75%           1.000000       0.000000     0.000000       0.000000   \n",
       "max          34.000000       1.000000     1.000000       1.000000   \n",
       "\n",
       "         cow_civil  ...  v2xcl_acjst  v2xcs_ccsi   v2x_freexp  v2xme_altinf  \\\n",
       "count  4567.000000  ...  9520.000000  9520.00000  9520.000000   9520.000000   \n",
       "mean      0.147580  ...     0.583364     0.57373     0.564268      0.553715   \n",
       "std       0.354722  ...     0.291367     0.31867     0.322586      0.330446   \n",
       "min       0.000000  ...     0.002000     0.00800     0.011000      0.009000   \n",
       "25%       0.000000  ...     0.318000     0.27375     0.250000      0.216000   \n",
       "50%       0.000000  ...     0.622000     0.63700     0.618000      0.679000   \n",
       "75%       0.000000  ...     0.850000     0.88500     0.871000      0.853000   \n",
       "max       1.000000  ...     0.997000     0.98300     0.992000      0.977000   \n",
       "\n",
       "        v2smgovdom  v2smgovfilcap  v2smgovfilprc  v2smgovshutcap  v2smgovshut  \\\n",
       "count  3858.000000    3858.000000    3858.000000     3858.000000  3858.000000   \n",
       "mean      0.101634      -0.086477       0.469499       -0.163223     0.602092   \n",
       "std       1.378806       1.268826       1.543879        1.286865     1.291013   \n",
       "min      -3.640000      -3.329000      -3.898000       -3.162000    -4.169000   \n",
       "25%      -0.920500      -1.042750      -0.666000       -1.160250    -0.376000   \n",
       "50%      -0.022000      -0.070000       0.866000       -0.215000     1.013000   \n",
       "75%       1.153750       0.961500       1.823000        0.918000     1.750000   \n",
       "max       2.877000       2.943000       2.601000        2.507000     2.004000   \n",
       "\n",
       "       v2xedvd_me_cent  \n",
       "count      9352.000000  \n",
       "mean          0.465796  \n",
       "std           0.322204  \n",
       "min           0.012000  \n",
       "25%           0.161000  \n",
       "50%           0.423000  \n",
       "75%           0.794000  \n",
       "max           0.981000  \n",
       "\n",
       "[8 rows x 105 columns]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.head(1)\n",
    "meta.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c4daa34-d1c6-4849-aab3-d9ae93edd7d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (628 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "  # Tokenize the text\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "    \n",
    "truncate_length = len(tokenized_text) - 512 + 2  # +2 to account for [CLS] and [SEP]\n",
    "        \n",
    "# Truncate the beginning and end of the text\n",
    "truncated_text = tokenized_text[truncate_length//2 : -truncate_length//2]\n",
    "\n",
    "marked_text = [\"[CLS] \"] + truncated_text + [\" [SEP]\"]\n",
    "# Add special tokens [CLS] and [SEP]\n",
    "        \n",
    "# Convert tokens to ids\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(marked_text)\n",
    "        \n",
    "# Create attention mask\n",
    "attention_mask = [1] * len(indexed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "db044cd0-5687-4339-b7be-e59127c251ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It              100\n",
      "privilege    19,800\n",
      "express       4,745\n",
      ",             1,145\n",
      "Mr            1,871\n",
      ".             1,263\n",
      "President     2,541\n",
      ",             1,714\n",
      "con           1,353\n",
      "##gratulations  1,583\n",
      "Afghanistan   6,241\n",
      "delegation    1,958\n",
      "election      2,030\n",
      ",             1,607\n",
      "just            119\n",
      "##ly          1,284\n",
      "unanimously   2,059\n",
      "voted         3,519\n",
      "Assembly      1,362\n",
      ".             1,169\n",
      "It            6,561\n",
      "also          7,616\n",
      "privilege       117\n",
      "extend        2,218\n",
      "fellow        2,174\n",
      "representatives 16,286\n",
      "greeting      9,113\n",
      "##s           3,519\n",
      "Royal           119\n",
      "Afghan        1,130\n",
      "Government    2,157\n",
      ",               117\n",
      "well         23,614\n",
      "sincere       7,279\n",
      "##st          3,681\n",
      "wishes          117\n",
      "success      11,565\n",
      "current         117\n",
      "session      21,820\n",
      "General      13,378\n",
      "Assembly     14,819\n",
      ".            10,774\n",
      "Our           3,235\n",
      "attachment    3,844\n",
      "United        1,311\n",
      "Nations       4,309\n",
      "Charter       6,551\n",
      "principles      119\n",
      "complete      1,109\n",
      "ad            7,533\n",
      "##herence     6,469\n",
      "principles   11,703\n",
      "human         2,166\n",
      "rights        6,887\n",
      "self          2,970\n",
      "-             3,768\n",
      "determination  6,551\n",
      "peoples         117\n",
      "based         1,209\n",
      "ideological   2,760\n",
      "grounds       1,692\n",
      "also          1,954\n",
      "result        4,912\n",
      "long            119\n",
      "experience    1,188\n",
      "free          7,533\n",
      "small         6,858\n",
      "country       1,912\n",
      "controversial  4,078\n",
      "events        2,019\n",
      "modern        1,583\n",
      "history       1,372\n",
      ".             2,182\n",
      "We              117\n",
      "believe       1,911\n",
      "peace         1,472\n",
      "world         3,501\n",
      "can           7,891\n",
      "secured       1,244\n",
      "bases         3,854\n",
      ",               117\n",
      "certain       7,762\n",
      "future        1,621\n",
      "prosperity    3,844\n",
      "depends       1,311\n",
      "peace         1,359\n",
      ".            11,610\n",
      "In           19,069\n",
      "saying          119\n",
      ",             1,188\n",
      "posing        1,362\n",
      "moral         2,741\n",
      "##ists       15,194\n",
      ",             7,243\n",
      "contrary      1,314\n",
      ",             4,030\n",
      "hum           1,201\n",
      "##ility         119\n",
      "expressing    1,109\n",
      "conviction   11,619\n",
      "fellow        5,172\n",
      "Member        2,170\n",
      "States        3,141\n",
      "attached     21,876\n",
      "principles    2,379\n",
      ".             1,735\n",
      "The           1,237\n",
      "attitude     21,876\n",
      "Afghanistan   1,314\n",
      "delegation    1,432\n",
      "previous        119\n",
      "sessions      4,516\n",
      "Assembly        117\n",
      "inspired      2,726\n",
      "principles    5,172\n",
      ",               117\n",
      "will          3,142\n",
      "continue      7,649\n",
      "case         13,542\n",
      "current       1,864\n",
      "session      19,967\n",
      ".             4,232\n",
      "This          2,645\n",
      "attitude      1,362\n",
      "characterized  3,035\n",
      "kind          9,175\n",
      "opposition    4,287\n",
      "towards       1,363\n",
      "country      17,274\n",
      "group           117\n",
      "countries     5,973\n",
      ",             9,535\n",
      "idea          1,892\n",
      "different    10,680\n",
      "basic           119\n",
      "ideal         1,284\n",
      "United       15,869\n",
      "Nations       2,812\n",
      ",             1,413\n",
      "distinction   2,052\n",
      "among           117\n",
      "Member        1,443\n",
      "States       19,173\n",
      "based         1,596\n",
      "geographical  5,442\n",
      "considerations  7,698\n",
      ".             1,348\n",
      "This          4,681\n",
      "world         2,019\n",
      "scene         1,583\n",
      "tremendous      119\n",
      "evolution     1,109\n",
      "last          4,574\n",
      "twelve        4,245\n",
      "years        21,839\n",
      ".            10,296\n",
      "The           1,244\n",
      "nationalist   3,854\n",
      "movements    12,500\n",
      "African       1,141\n",
      "Asian         1,436\n",
      "continents    5,136\n",
      "natural         117\n",
      "European     22,647\n",
      "American      1,268\n",
      "continents    2,365\n",
      "last         25,946\n",
      "century       1,632\n",
      ".             3,790\n",
      "Thus          1,145\n",
      ",            12,839\n",
      "supported     8,982\n",
      "movements    13,542\n",
      ",             1,632\n",
      "basis         3,794\n",
      "objective       117\n",
      "appreciation  1,244\n",
      "fact          2,325\n",
      "sincere         119\n",
      "desire        2,695\n",
      "problems     19,967\n",
      "world         1,193\n",
      "settled      14,255\n",
      "mutual       14,867\n",
      "understanding  7,926\n",
      "good          6,951\n",
      "##will          117\n",
      ",             5,136\n",
      "violent       1,723\n",
      "reactions     1,861\n",
      "blood         2,740\n",
      "##shed          119\n",
      ".             1,284\n",
      "We            4,663\n",
      "likewise      1,244\n",
      "follow        3,854\n",
      "line          7,866\n",
      "today         1,675\n",
      ",             1,362\n",
      "without       2,645\n",
      "antagonist    1,737\n",
      "##ic          1,242\n",
      "fan          22,168\n",
      "##atic          119\n",
      "##al          1,284\n",
      "feelings      1,145\n",
      "towards       4,663\n",
      "country       1,769\n",
      ".             2,645\n",
      "The           8,277\n",
      "independence    117\n",
      "Federation    1,569\n",
      "Malaya        1,741\n",
      "admission     2,670\n",
      "United        2,645\n",
      "Nations       1,505\n",
      "constitute    1,648\n",
      "one             117\n",
      "best          2,510\n",
      "examples      2,463\n",
      ",             4,485\n",
      "justification  2,335\n",
      "right        15,146\n",
      "##ful        16,334\n",
      "aspirations   1,362\n",
      "great         2,645\n",
      "nation          119\n",
      "also          1,753\n",
      "generous     22,922\n",
      "gesture      17,277\n",
      "appreciation  7,866\n",
      "great           117\n",
      "Power         1,649\n",
      ",               117\n",
      "United        2,255\n",
      "Kingdom       8,856\n",
      ".             2,999\n",
      "Both          7,243\n",
      "sincere       1,362\n",
      "##ly            119\n",
      "con          18,101\n",
      "##gra           117\n",
      "##tu          4,663\n",
      "##lated       1,632\n",
      ",             6,534\n",
      "examples        117\n",
      "followed        195\n",
      "similar      13,003\n",
      "cases        17,115\n",
      ".             2,909\n",
      "We              118\n",
      "realize       1,615\n",
      "United        6,970\n",
      "Nations       3,878\n",
      "difficulties    117\n",
      "present       2,065\n",
      "world         1,977\n",
      "problems      1,373\n",
      "considered    3,507\n",
      "many          9,286\n",
      "perspectives  4,030\n",
      ".             1,201\n",
      "We            2,403\n",
      "also            119\n",
      "realize       1,188\n",
      "human         2,612\n",
      "problems      2,810\n",
      "complicated   2,554\n",
      ",             5,070\n",
      "national        132\n",
      "political     2,059\n",
      "economic     14,740\n",
      "problems      8,050\n",
      "play         21,634\n",
      "role          6,551\n",
      ",             4,840\n",
      "individual   11,171\n",
      "problem       4,103\n",
      "regarded      6,044\n",
      "complete      9,414\n",
      "detachment    1,242\n",
      "currents      7,866\n",
      "world           119\n",
      "problems      3,458\n",
      ".            11,769\n",
      "Not           3,121\n",
      "##with       20,279\n",
      "##standing    1,359\n",
      "difficulties  8,418\n",
      ",             7,983\n",
      "however       1,362\n",
      ",               117\n",
      "reason        3,519\n",
      "appreciate      117\n",
      "normal        4,174\n",
      "evolution     4,444\n",
      "world         6,183\n",
      ".             7,514\n",
      "Fortunately   5,502\n",
      ",             4,840\n",
      "realize      11,171\n",
      "great           119\n",
      "Organization  2,994\n",
      ",             1,451\n",
      "z             1,141\n",
      "##eal         1,366\n",
      "eminent       5,857\n",
      "Secretary     1,330\n",
      "-             1,594\n",
      "General       2,498\n",
      "devoted       1,720\n",
      "officials     2,335\n",
      ",             1,126\n",
      "gone          2,605\n",
      "forward      20,473\n",
      "along         1,891\n",
      "path          1,363\n",
      "traced          117\n",
      "twelve        3,869\n",
      "years           118\n",
      "ago           2,712\n",
      ".             1,297\n",
      "This            118\n",
      "cause         3,229\n",
      "hope          1,297\n",
      "evidence        119\n",
      "progress      1,284\n",
      ";            13,099\n",
      "believe       1,116\n",
      "devotion     19,034\n",
      "ad            8,333\n",
      "##herence     4,438\n",
      "principles    2,222\n",
      "spirit        4,989\n",
      "Charter      16,286\n",
      "shall         1,234\n",
      "gradually     1,546\n",
      "overcome      1,336\n",
      "many          1,686\n",
      "difficulties  2,030\n",
      ".             3,790\n",
      "Our           1,362\n",
      "op              119\n",
      "##ti          1,284\n",
      "##mism        4,309\n",
      "based         7,181\n",
      "consciousness  6,170\n",
      "peoples      10,261\n",
      "world         1,218\n",
      ",             5,317\n",
      "peace           117\n",
      ",            20,061\n",
      "alternative   1,149\n",
      "ultimately   21,932\n",
      "identify     27,697\n",
      "respective    2,731\n",
      "policies      1,583\n",
      "spirit          119\n",
      "Charter       1,284\n",
      ".             1,774\n",
      "Each          8,333\n",
      "every           117\n",
      "one           2,407\n",
      "us              117\n",
      "convinced     1,579\n",
      "another         117\n",
      "war           6,472\n",
      "bring         4,438\n",
      "nothing      12,363\n",
      "complete        118\n",
      "an            1,734\n",
      "##ni            119\n",
      "##hil        18,101\n",
      "##ation         117\n",
      "good          9,397\n",
      ",             1,700\n",
      "worth        27,887\n",
      "-            25,449\n",
      "beautiful     7,289\n",
      "life          4,132\n",
      "-             1,314\n",
      "perhaps       3,049\n",
      "life          1,201\n",
      ".               117\n",
      "We            2,108\n",
      "Afghan        1,160\n",
      "##s           1,362\n",
      "ambition      8,755\n",
      "preserve        119\n",
      "freedom       1,706\n",
      "try           3,689\n",
      "ensure        2,030\n",
      "prosperity    3,708\n",
      "people        1,583\n",
      "order         1,444\n",
      "may           1,619\n",
      "live          5,052\n",
      "modern        1,872\n",
      "nation        2,182\n",
      "world           117\n",
      ".             9,473\n",
      "We            3,531\n",
      "attached      4,256\n",
      "traditions      119\n",
      "spiritual     1,284\n",
      "legacy        5,958\n",
      "well          8,856\n",
      "Constitution  4,301\n",
      ",             5,052\n",
      "spontaneous   1,460\n",
      "out           1,244\n",
      "##gro         3,854\n",
      "##wth           132\n",
      "nature        8,856\n",
      "country       2,860\n",
      ".               117\n",
      "We            1,218\n",
      "trying        3,767\n",
      "preserve      1,549\n",
      ",             1,366\n",
      "ready         2,739\n",
      ",             1,884\n",
      "always          118\n",
      ",             2,805\n",
      "defend        1,362\n",
      "freedom       6,534\n",
      "integrity       119\n",
      "-             6,469\n",
      "words         6,616\n",
      ".             9,441\n",
      "Fortunately   3,433\n",
      ",             1,835\n",
      "stability     5,408\n",
      "position      2,645\n",
      "sincerity       117\n",
      "neutrality    1,632\n",
      "tested        1,353\n",
      "proved          119\n",
      "last          1,284\n",
      "fifth         1,793\n",
      "years         1,763\n",
      ",               117\n",
      "especially    2,244\n",
      "two             117\n",
      "world         7,098\n",
      "wars          1,242\n",
      ".             2,645\n",
      "To            2,904\n",
      "develop       7,624\n",
      "modern          117\n",
      "##ize         1,329\n",
      "country       1,363\n",
      "need          4,158\n",
      "support         117\n",
      "assistance    5,566\n",
      "developed     4,301\n",
      "countries     1,494\n",
      ",             2,053\n",
      "grateful        117\n",
      "receive       9,441\n",
      "aid           2,086\n",
      ".            14,255\n",
      "We            6,617\n",
      "greatly       4,567\n",
      "appreciate    2,116\n",
      "technical       119\n",
      "assistance    1,284\n",
      "received      1,774\n",
      "United        1,208\n",
      "Nations         117\n",
      ";             4,103\n",
      "appreciate    2,222\n",
      "value         2,174\n",
      ",               117\n",
      "well          7,098\n",
      "opportunity   2,645\n",
      "given         2,086\n",
      "us              117\n",
      "closer        3,142\n",
      "co            7,649\n",
      "-               117\n",
      "operation     8,362\n",
      "world         1,643\n",
      "Organization  1,874\n",
      ".             9,380\n",
      "Afghanistan  19,237\n",
      "believes      1,181\n",
      "peaceful      9,486\n",
      "settlement    6,551\n",
      "international  1,268\n",
      "differences   5,299\n",
      "problems        119\n",
      ",             1,130\n",
      "great         2,458\n",
      "small           117\n",
      ".            12,747\n",
      "We            2,645\n",
      "tried         1,362\n",
      "past          9,802\n",
      ",            14,368\n",
      "success         117\n",
      ",            10,010\n",
      "settle        3,835\n",
      "many          8,362\n",
      "problems     23,242\n",
      "direct        2,645\n",
      "negotiations    117\n",
      ",             3,084\n",
      "use           4,679\n",
      "good         14,255\n",
      "offices       6,617\n",
      ",               100\n"
     ]
    }
   ],
   "source": [
    "#This one prints out the tokenized word pieces, along with indices. \n",
    "\n",
    "#for tup in zip(tokenized_text, indexed_tokens):\n",
    "#    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b3a1c68-18f3-4be4-8e46-21987d268dfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pad sequences to max_seq_length\n",
    "if len(indexed_tokens) < 512:\n",
    "    indexed_tokens.append(0)\n",
    "    attention_mask.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1ff13f7-82ac-416c-aa35-dfa1a7aaabe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert lists to PyTorch tensors\n",
    "tokenized_texts = []\n",
    "tokens_tensors = []\n",
    "attention_masks = []\n",
    "\n",
    "    \n",
    "tokens_tensors.append(torch.tensor(indexed_tokens))\n",
    "attention_masks.append(torch.tensor(attention_mask))\n",
    "tokenized_texts.append(tokenized_text)\n",
    "\n",
    "# Convert lists to PyTorch tensors\n",
    "tokens_tensors = torch.stack(tokens_tensors)\n",
    "attention_masks = torch.stack(attention_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84e7635d-1071-4ab9-a9ef-dee27f894f52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the BERT model\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=tokens_tensors.view(-1, tokens_tensors.size(-1)), attention_mask=attention_masks.view(-1, attention_masks.size(-1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149d9dd8-1a4d-44f5-b69b-224e5b6b26c8",
   "metadata": {},
   "source": [
    "# Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "ab5de7a9-fded-4160-b5d7-f7c72bddbc58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512,)\n",
      "(512, 768)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "      <th>term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.620744</td>\n",
       "      <td>0.348515</td>\n",
       "      <td>0.351208</td>\n",
       "      <td>-0.195764</td>\n",
       "      <td>0.667364</td>\n",
       "      <td>0.406018</td>\n",
       "      <td>0.473575</td>\n",
       "      <td>0.795273</td>\n",
       "      <td>0.415110</td>\n",
       "      <td>0.341460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205765</td>\n",
       "      <td>0.405889</td>\n",
       "      <td>0.731192</td>\n",
       "      <td>-0.086030</td>\n",
       "      <td>-0.744767</td>\n",
       "      <td>-0.107819</td>\n",
       "      <td>0.911729</td>\n",
       "      <td>0.347175</td>\n",
       "      <td>0.335730</td>\n",
       "      <td>believe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>-0.181749</td>\n",
       "      <td>-0.923341</td>\n",
       "      <td>-0.502419</td>\n",
       "      <td>0.267246</td>\n",
       "      <td>-0.467946</td>\n",
       "      <td>-0.439854</td>\n",
       "      <td>-0.828056</td>\n",
       "      <td>0.443169</td>\n",
       "      <td>-0.869504</td>\n",
       "      <td>-2.072055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047809</td>\n",
       "      <td>0.623744</td>\n",
       "      <td>-0.455522</td>\n",
       "      <td>0.004116</td>\n",
       "      <td>-0.674838</td>\n",
       "      <td>-0.335369</td>\n",
       "      <td>0.577260</td>\n",
       "      <td>-0.633902</td>\n",
       "      <td>0.241060</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>-0.731771</td>\n",
       "      <td>0.355430</td>\n",
       "      <td>0.949808</td>\n",
       "      <td>0.068705</td>\n",
       "      <td>1.059148</td>\n",
       "      <td>-0.031420</td>\n",
       "      <td>1.051520</td>\n",
       "      <td>0.635568</td>\n",
       "      <td>0.669277</td>\n",
       "      <td>0.817705</td>\n",
       "      <td>...</td>\n",
       "      <td>0.234071</td>\n",
       "      <td>0.340437</td>\n",
       "      <td>0.762802</td>\n",
       "      <td>-0.045645</td>\n",
       "      <td>-1.094815</td>\n",
       "      <td>-0.255272</td>\n",
       "      <td>0.648554</td>\n",
       "      <td>0.450855</td>\n",
       "      <td>0.231913</td>\n",
       "      <td>believe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>-0.023267</td>\n",
       "      <td>-0.744896</td>\n",
       "      <td>-0.409934</td>\n",
       "      <td>0.549873</td>\n",
       "      <td>-0.542924</td>\n",
       "      <td>-0.453240</td>\n",
       "      <td>-1.005128</td>\n",
       "      <td>0.303455</td>\n",
       "      <td>-0.629427</td>\n",
       "      <td>-1.995248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199423</td>\n",
       "      <td>0.312459</td>\n",
       "      <td>-0.636080</td>\n",
       "      <td>-0.016288</td>\n",
       "      <td>-0.437767</td>\n",
       "      <td>-0.387403</td>\n",
       "      <td>0.471238</td>\n",
       "      <td>-0.805941</td>\n",
       "      <td>-0.266752</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "16  -0.620744  0.348515  0.351208 -0.195764  0.667364  0.406018  0.473575   \n",
       "164 -0.181749 -0.923341 -0.502419  0.267246 -0.467946 -0.439854 -0.828056   \n",
       "270 -0.731771  0.355430  0.949808  0.068705  1.059148 -0.031420  1.051520   \n",
       "489 -0.023267 -0.744896 -0.409934  0.549873 -0.542924 -0.453240 -1.005128   \n",
       "\n",
       "            7         8         9  ...       759       760       761  \\\n",
       "16   0.795273  0.415110  0.341460  ...  0.205765  0.405889  0.731192   \n",
       "164  0.443169 -0.869504 -2.072055  ...  0.047809  0.623744 -0.455522   \n",
       "270  0.635568  0.669277  0.817705  ...  0.234071  0.340437  0.762802   \n",
       "489  0.303455 -0.629427 -1.995248  ...  0.199423  0.312459 -0.636080   \n",
       "\n",
       "          762       763       764       765       766       767     term  \n",
       "16  -0.086030 -0.744767 -0.107819  0.911729  0.347175  0.335730  believe  \n",
       "164  0.004116 -0.674838 -0.335369  0.577260 -0.633902  0.241060    right  \n",
       "270 -0.045645 -1.094815 -0.255272  0.648554  0.450855  0.231913  believe  \n",
       "489 -0.016288 -0.437767 -0.387403  0.471238 -0.805941 -0.266752    right  \n",
       "\n",
       "[4 rows x 769 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_words = pd.Series(marked_text, name='term')\n",
    "print(pd_words.shape)\n",
    "\n",
    "hidden_states = outputs[2][0].squeeze().numpy()\n",
    "print(hidden_states.shape)\n",
    "\n",
    "df_outputs = pd.DataFrame(hidden_states)\n",
    "df_outputs[\"term\"] = pd_words\n",
    "\n",
    "\n",
    "df_outputs.loc[(df_outputs['term'] == \"right\") | (df_outputs['term'] == \"believe\")]\n",
    "\n",
    "#Each column represents each term. Dimension is 768 X 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "be60b6c8-81f2-439b-9bfa-1b41b9c2d0bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices = list((df_outputs['term'] == \"right\") | (df_outputs['term'] == \"believe\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "111eed46-0a85-479c-919e-aa8c761f4a53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 769)\n"
     ]
    }
   ],
   "source": [
    "subset = df_outputs[(df_outputs['term'] == \"right\") | (df_outputs['term'] == \"believe\")]\n",
    "subset_np = np.array(subset)\n",
    "print(subset_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "d0c793ce-8168-4c23-ab77-85294e853dfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n",
      "Cosine Similarities: 0.882696943581341\n"
     ]
    }
   ],
   "source": [
    "# Calculate cosine similarity row-wise\n",
    "\n",
    "A = np.array(subset_np[1,:-1])\n",
    "print(A.shape)\n",
    "\n",
    "B = np.array(subset_np[3,:-1])\n",
    "cosine = np.sum(A * B) / (np.linalg.norm(A) * np.linalg.norm(B))\n",
    "\n",
    "# Print cosine similarities\n",
    "print(\"Cosine Similarities:\", cosine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "686e08e4-4670-4ab7-9f59-943138e0d081",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['##ci', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', '-', '-', '-', '-', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', ';', 'Afghanistan', 'Charter', 'Fortunately', 'In', 'Member', 'Nations', 'Nations', 'Nations', 'Organization', 'States', 'The', 'The', 'This', 'This', 'United', 'United', 'United', 'United', 'We', 'We', 'We', 'We', 'We', 'We', 'We', 'We', 'We', 'also', 'also', 'appreciate', 'appreciate', 'appreciation', 'assistance', 'attached', 'attitude', 'based', 'basis', 'believe', 'complete', 'con', 'con', 'continents', 'countries', 'country', 'country', 'country', 'country', 'difficulties', 'difficulties', 'evolution', 'examples', 'freedom', 'future', 'good', 'good', 'great', 'great', 'great', 'last', 'last', 'life', 'many', 'many', 'means', 'modern', 'modern', 'movements', 'nation', 'objective', 'one', 'peace', 'peace', 'peaceful', 'preserve', 'principles', 'principles', 'principles', 'problems', 'problems', 'problems', 'problems', 'problems', 'problems', 'problems', 'problems', 'problems', 'prosperity', 'realize', 'realize', 'right', 'settle', 'shall', 'sincere', 'small', 'spirit', 'technical', 'towards', 'try', 'trying', 'twelve', 'un', 'us', 'well', 'world', 'world', 'world', 'world', 'world', 'world', 'world', 'world', 'world', 'world', 'years', 'years']\n"
     ]
    }
   ],
   "source": [
    "# find duplicate rows\n",
    "duplicate_rows = df_outputs[df_outputs.duplicated('term')].sort_values('term')\n",
    "\n",
    "duplicate_rows=pd.DataFrame(duplicate_rows)\n",
    "# print duplicate rows\n",
    "duplicate_rows['term']\n",
    "\n",
    "freq_table = pd.crosstab(duplicate_rows['term'], 'no_of_duplicates') \n",
    "print(list(duplicate_rows['term']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702365f6-3dc3-422b-a229-b4cf8cc9376b",
   "metadata": {},
   "source": [
    "- I don't suspect a significant semantic differences between duplicates. Possible problems could've arisen with words like \"right,\" but cosine similarity between two \"right\"s were 0.88. That score was as high as similarity score between duplicated \"We,\" which scored 0.85. \n",
    "- With that, we decided to group duplicates into one by averaging them out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "027d341f-f0dc-4f92-ac36-92dc4d3aa53b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0         1         2         3         4         5         6    \\\n",
      "term                                                                            \n",
      " [SEP]  -0.409629 -0.746030  0.954548 -0.989581 -0.666579 -0.456214 -0.482902   \n",
      "##al    -0.524131  0.479899 -0.476182 -0.390430  0.571847  0.344540 -0.890469   \n",
      "##atic  -1.258919 -1.749650  0.189675 -1.009726  1.250488 -0.127454 -0.174475   \n",
      "##ation -0.321686 -0.599758  0.114046 -0.144932  0.953009 -0.936072  0.038542   \n",
      "##ci    -0.209563  0.008983  0.226166 -0.037539 -0.431536 -0.854313 -0.930581   \n",
      "...           ...       ...       ...       ...       ...       ...       ...   \n",
      "words    0.397322 -0.091103  0.350278  0.339480  0.188769 -0.122880  0.119570   \n",
      "world    0.991384 -0.232762  0.670588  0.508774  0.526652  0.077158 -0.581935   \n",
      "worth   -0.107531  0.469970 -0.674297  0.848895  0.115055  0.818750 -0.049695   \n",
      "years    0.478471 -0.254025 -0.260787  0.681818 -0.097176  0.219434 -0.531882   \n",
      "z        0.215062 -0.684214 -0.219748 -0.830821  0.585531 -0.721144 -0.274569   \n",
      "\n",
      "              7         8         9    ...       758       759       760  \\\n",
      "term                                   ...                                 \n",
      " [SEP]  -0.474517 -0.324448 -0.554452  ...  0.937598  0.148158  0.841253   \n",
      "##al    -0.778739 -0.369768 -0.369769  ... -0.796855 -0.407300  0.098990   \n",
      "##atic   0.132727  0.210920  0.159041  ...  0.256487  0.588991 -1.045582   \n",
      "##ation  0.732640  0.667231 -0.173093  ... -0.485267  0.513310  0.546876   \n",
      "##ci    -0.289056  0.014480  0.836836  ...  0.035582 -0.291024  0.075818   \n",
      "...           ...       ...       ...  ...       ...       ...       ...   \n",
      "words   -0.366882  0.528809  0.175150  ... -0.813381 -0.366494  0.197109   \n",
      "world    0.377466  0.073109  0.119594  ... -0.505561  0.136024  0.009123   \n",
      "worth   -0.560952 -0.509267 -0.288416  ...  0.032292 -0.336956  0.691349   \n",
      "years   -0.517204  0.729554  0.117572  ...  0.398866  0.446946 -0.115180   \n",
      "z        0.207120  0.271554 -1.113561  ... -0.068485 -0.185102 -0.254614   \n",
      "\n",
      "              761       762       763       764       765       766       767  \n",
      "term                                                                           \n",
      " [SEP]  -0.650847 -0.348678 -0.864865 -0.147155  0.445907  0.226717 -1.860195  \n",
      "##al     0.707968 -0.098804 -1.176002  0.376283  0.099251 -0.782967 -0.062336  \n",
      "##atic  -0.885318 -0.134264 -0.281723  0.578229 -0.318711 -0.002958  0.664755  \n",
      "##ation  0.632246  0.262685 -0.174507  0.236400 -0.632015 -0.797872 -1.417162  \n",
      "##ci     0.033696  0.396918 -0.968211 -0.381200  0.396398  0.112123 -0.490416  \n",
      "...           ...       ...       ...       ...       ...       ...       ...  \n",
      "words   -0.413089  0.482895 -0.190066 -0.568232  0.127709 -0.095941  0.127358  \n",
      "world    0.495411  0.342043 -1.043479 -0.131786 -0.333381 -0.390346 -0.141330  \n",
      "worth    0.267413 -0.045436  0.660112  1.184719 -0.789721  0.254513 -0.662614  \n",
      "years   -0.216491 -0.296925  0.555254 -0.139911 -0.880707 -0.042803  0.468518  \n",
      "z       -0.406646 -0.101535 -0.219950 -0.111580 -0.428931 -0.333112 -0.023003  \n",
      "\n",
      "[321 rows x 768 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index([' [SEP]', '##al', '##atic', '##ation', '##ci', '##d', '##dice', '##eal',\n",
       "       '##ful', '##gra',\n",
       "       ...\n",
       "       'war', 'wars', 'well', 'will', 'without', 'words', 'world', 'worth',\n",
       "       'years', 'z'],\n",
       "      dtype='object', name='term', length=321)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_outputs_embedding = df_outputs.groupby(['term']).mean()\n",
    "print(df_outputs_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "360b0495-40cc-4460-8b46-bce8094e7f16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_outputs_embedding.to_csv(\"../../../output/embeddings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f34578e-0fa4-46dc-b671-2ccdbfd3ea54",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "49bc24ff-35a8-461f-a5a9-7ce55b6d450c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 1 (initial embeddings + 12 BERT layers)\n",
      "Number of batches: 512\n",
      "Number of tokens: 768\n",
      "Type of hidden_states: <class 'torch.Tensor'>\n",
      "Tensor shape for each layer:  torch.Size([512, 768])\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of layers:\", len(hidden_states), \"(initial embeddings + 12 BERT layers)\")\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(hidden_states[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print('Type of hidden_states:', type(hidden_states))\n",
    "\n",
    "print('Tensor shape for each layer: ', hidden_states[0].size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d3a378-4ab8-43fa-adc4-59fb2160cdeb",
   "metadata": {},
   "source": [
    "## Cosine Similarity between two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "9c448262-a45e-4fc4-a0e5-cea6b6672a09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 768)\n",
      "Cosine Similarity between 'peace' and 'world': [[0.01612619]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "term_a = \"peace\"\n",
    "term_b = \"world\"\n",
    "# Extract the mean embeddings for the terms \"right\" and \"believe\"\n",
    "embedding_a = df_outputs_embedding.loc[term_a].values.reshape(1, -1)\n",
    "embedding_b = df_outputs_embedding.loc[term_b].values.reshape(1, -1)\n",
    "print(embedding_a.shape)\n",
    "# Compute cosine similarity between the two mean embeddings\n",
    "cosine_similarity = cosine_similarity(embedding_a, embedding_b)\n",
    "#range=[0,1]\n",
    "print(f\"Cosine Similarity between '{term_a}' and '{term_b}': {cosine_similarity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b78910-126f-4395-bb01-3d7fa2da758e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Limitations of BERT\n",
    "\n",
    "## 1) **Anisotropy**\n",
    "I expected \"peace\" and \"world\" to show greater distance than a pair of \"war\" and \"peace.\" But they didn't. Not only did these two, but literature shows that if we take any random words, cosines will be high as close as 1 ([Jurafsky and Martin 2024](https://web.stanford.edu/~jurafsky/slp3/)). Apparently, we need to do additional transformations with embeddings extracted from BERT. [Timkey et al. (2021)](https://aclanthology.org/2021.emnlp-main.372/) points out such tendency can be attenuated by standardizing the vectors and reducing the impact of outliers. By outliers Timkey et al. (2021) mean few dimensions of embedding that have high variance.\n",
    "\n",
    "Some people ([Li et al. 2020](https://aclanthology.org/2020.emnlp-main.733.pdf)) point out BERT's anisotropic characteristic results in underperformance in sentence similarity compared to GloVe embeddings. They further point out that the last layer of BERT is not appropriate for similarity metrics, given their non-smoothing characteristic.\n",
    "\n",
    "## 2) **Cross-document comparison**\n",
    "This is our hypothesis, but I suspect that mapping of words onto vector space is not linear. Even when mapping is anisotropic, it shouldn't pose a problem if the main goal is to simply cross-compare distances **between** documents. However, the same word that shows up in two different documents get different vectors. This context-specificity is a double-edged sword, in that it allows us to distinguish nuances, but also prevents us from doing comparison in a consistent manner. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "674d55da-89b7-4a9b-9263-6533956c95c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>term</th>\n",
       "      <th>[SEP]</th>\n",
       "      <th>##al</th>\n",
       "      <th>##atic</th>\n",
       "      <th>##ation</th>\n",
       "      <th>##ci</th>\n",
       "      <th>##d</th>\n",
       "      <th>##dice</th>\n",
       "      <th>##eal</th>\n",
       "      <th>##ful</th>\n",
       "      <th>##gra</th>\n",
       "      <th>...</th>\n",
       "      <th>war</th>\n",
       "      <th>wars</th>\n",
       "      <th>well</th>\n",
       "      <th>will</th>\n",
       "      <th>without</th>\n",
       "      <th>words</th>\n",
       "      <th>world</th>\n",
       "      <th>worth</th>\n",
       "      <th>years</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>[SEP]</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026143</td>\n",
       "      <td>-0.016379</td>\n",
       "      <td>0.175157</td>\n",
       "      <td>0.075565</td>\n",
       "      <td>0.089596</td>\n",
       "      <td>0.150333</td>\n",
       "      <td>0.035554</td>\n",
       "      <td>-0.033455</td>\n",
       "      <td>0.084167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028284</td>\n",
       "      <td>0.023479</td>\n",
       "      <td>0.060025</td>\n",
       "      <td>0.053393</td>\n",
       "      <td>0.043304</td>\n",
       "      <td>0.035146</td>\n",
       "      <td>-0.019725</td>\n",
       "      <td>0.060074</td>\n",
       "      <td>0.073359</td>\n",
       "      <td>0.093997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##al</th>\n",
       "      <td>0.026143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.093010</td>\n",
       "      <td>0.136414</td>\n",
       "      <td>0.062659</td>\n",
       "      <td>0.156533</td>\n",
       "      <td>0.072606</td>\n",
       "      <td>0.074310</td>\n",
       "      <td>-0.044487</td>\n",
       "      <td>0.097046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170718</td>\n",
       "      <td>0.132057</td>\n",
       "      <td>0.182142</td>\n",
       "      <td>0.204611</td>\n",
       "      <td>0.234805</td>\n",
       "      <td>0.057567</td>\n",
       "      <td>0.339189</td>\n",
       "      <td>0.024444</td>\n",
       "      <td>0.080924</td>\n",
       "      <td>0.153247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##atic</th>\n",
       "      <td>-0.016379</td>\n",
       "      <td>0.093010</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009135</td>\n",
       "      <td>0.043850</td>\n",
       "      <td>0.015896</td>\n",
       "      <td>0.055103</td>\n",
       "      <td>0.017012</td>\n",
       "      <td>-0.057231</td>\n",
       "      <td>0.017846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114068</td>\n",
       "      <td>-0.000627</td>\n",
       "      <td>0.057127</td>\n",
       "      <td>0.094940</td>\n",
       "      <td>0.084769</td>\n",
       "      <td>0.002506</td>\n",
       "      <td>0.064155</td>\n",
       "      <td>-0.061443</td>\n",
       "      <td>0.042701</td>\n",
       "      <td>0.043055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##ation</th>\n",
       "      <td>0.175157</td>\n",
       "      <td>0.136414</td>\n",
       "      <td>0.009135</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.104572</td>\n",
       "      <td>0.145229</td>\n",
       "      <td>0.095189</td>\n",
       "      <td>0.118794</td>\n",
       "      <td>0.037401</td>\n",
       "      <td>0.157255</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158142</td>\n",
       "      <td>0.054786</td>\n",
       "      <td>0.302470</td>\n",
       "      <td>0.106522</td>\n",
       "      <td>0.163783</td>\n",
       "      <td>0.150252</td>\n",
       "      <td>0.179236</td>\n",
       "      <td>0.088714</td>\n",
       "      <td>0.161013</td>\n",
       "      <td>0.495971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##ci</th>\n",
       "      <td>0.075565</td>\n",
       "      <td>0.062659</td>\n",
       "      <td>0.043850</td>\n",
       "      <td>0.104572</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.106008</td>\n",
       "      <td>-0.002979</td>\n",
       "      <td>0.077277</td>\n",
       "      <td>0.040556</td>\n",
       "      <td>0.069917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096960</td>\n",
       "      <td>0.013551</td>\n",
       "      <td>0.095918</td>\n",
       "      <td>0.070276</td>\n",
       "      <td>0.153076</td>\n",
       "      <td>0.074848</td>\n",
       "      <td>0.114379</td>\n",
       "      <td>0.117168</td>\n",
       "      <td>0.045349</td>\n",
       "      <td>0.138723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>words</th>\n",
       "      <td>0.035146</td>\n",
       "      <td>0.057567</td>\n",
       "      <td>0.002506</td>\n",
       "      <td>0.150252</td>\n",
       "      <td>0.074848</td>\n",
       "      <td>0.162133</td>\n",
       "      <td>0.077808</td>\n",
       "      <td>0.167498</td>\n",
       "      <td>-0.055219</td>\n",
       "      <td>-0.034550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.191374</td>\n",
       "      <td>0.067224</td>\n",
       "      <td>0.094747</td>\n",
       "      <td>0.119390</td>\n",
       "      <td>0.149054</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.108471</td>\n",
       "      <td>0.062540</td>\n",
       "      <td>0.099068</td>\n",
       "      <td>0.380922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>world</th>\n",
       "      <td>-0.019725</td>\n",
       "      <td>0.339189</td>\n",
       "      <td>0.064155</td>\n",
       "      <td>0.179236</td>\n",
       "      <td>0.114379</td>\n",
       "      <td>0.163717</td>\n",
       "      <td>0.105674</td>\n",
       "      <td>0.110620</td>\n",
       "      <td>-0.007374</td>\n",
       "      <td>0.079482</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174021</td>\n",
       "      <td>0.117663</td>\n",
       "      <td>0.186013</td>\n",
       "      <td>0.149951</td>\n",
       "      <td>0.168116</td>\n",
       "      <td>0.108471</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.027870</td>\n",
       "      <td>0.244823</td>\n",
       "      <td>0.253086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worth</th>\n",
       "      <td>0.060074</td>\n",
       "      <td>0.024444</td>\n",
       "      <td>-0.061443</td>\n",
       "      <td>0.088714</td>\n",
       "      <td>0.117168</td>\n",
       "      <td>0.017305</td>\n",
       "      <td>-0.018457</td>\n",
       "      <td>-0.027766</td>\n",
       "      <td>0.021058</td>\n",
       "      <td>0.040095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005939</td>\n",
       "      <td>-0.029942</td>\n",
       "      <td>0.072026</td>\n",
       "      <td>-0.001627</td>\n",
       "      <td>0.058985</td>\n",
       "      <td>0.062540</td>\n",
       "      <td>0.027870</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.051174</td>\n",
       "      <td>0.046957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>years</th>\n",
       "      <td>0.073359</td>\n",
       "      <td>0.080924</td>\n",
       "      <td>0.042701</td>\n",
       "      <td>0.161013</td>\n",
       "      <td>0.045349</td>\n",
       "      <td>0.239735</td>\n",
       "      <td>0.078811</td>\n",
       "      <td>0.116394</td>\n",
       "      <td>-0.057514</td>\n",
       "      <td>0.108901</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123907</td>\n",
       "      <td>0.093873</td>\n",
       "      <td>0.154401</td>\n",
       "      <td>0.097526</td>\n",
       "      <td>0.094451</td>\n",
       "      <td>0.099068</td>\n",
       "      <td>0.244823</td>\n",
       "      <td>0.051174</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.212903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z</th>\n",
       "      <td>0.093997</td>\n",
       "      <td>0.153247</td>\n",
       "      <td>0.043055</td>\n",
       "      <td>0.495971</td>\n",
       "      <td>0.138723</td>\n",
       "      <td>0.244000</td>\n",
       "      <td>0.199807</td>\n",
       "      <td>0.263103</td>\n",
       "      <td>-0.002866</td>\n",
       "      <td>0.113228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.321353</td>\n",
       "      <td>0.102310</td>\n",
       "      <td>0.393839</td>\n",
       "      <td>0.151623</td>\n",
       "      <td>0.276738</td>\n",
       "      <td>0.380922</td>\n",
       "      <td>0.253086</td>\n",
       "      <td>0.046957</td>\n",
       "      <td>0.212903</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>321 rows × 321 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "term        [SEP]      ##al    ##atic   ##ation      ##ci       ##d    ##dice  \\\n",
       "term                                                                            \n",
       " [SEP]   1.000000  0.026143 -0.016379  0.175157  0.075565  0.089596  0.150333   \n",
       "##al     0.026143  1.000000  0.093010  0.136414  0.062659  0.156533  0.072606   \n",
       "##atic  -0.016379  0.093010  1.000000  0.009135  0.043850  0.015896  0.055103   \n",
       "##ation  0.175157  0.136414  0.009135  1.000000  0.104572  0.145229  0.095189   \n",
       "##ci     0.075565  0.062659  0.043850  0.104572  1.000000  0.106008 -0.002979   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "words    0.035146  0.057567  0.002506  0.150252  0.074848  0.162133  0.077808   \n",
       "world   -0.019725  0.339189  0.064155  0.179236  0.114379  0.163717  0.105674   \n",
       "worth    0.060074  0.024444 -0.061443  0.088714  0.117168  0.017305 -0.018457   \n",
       "years    0.073359  0.080924  0.042701  0.161013  0.045349  0.239735  0.078811   \n",
       "z        0.093997  0.153247  0.043055  0.495971  0.138723  0.244000  0.199807   \n",
       "\n",
       "term        ##eal     ##ful     ##gra  ...       war      wars      well  \\\n",
       "term                                   ...                                 \n",
       " [SEP]   0.035554 -0.033455  0.084167  ...  0.028284  0.023479  0.060025   \n",
       "##al     0.074310 -0.044487  0.097046  ...  0.170718  0.132057  0.182142   \n",
       "##atic   0.017012 -0.057231  0.017846  ...  0.114068 -0.000627  0.057127   \n",
       "##ation  0.118794  0.037401  0.157255  ...  0.158142  0.054786  0.302470   \n",
       "##ci     0.077277  0.040556  0.069917  ...  0.096960  0.013551  0.095918   \n",
       "...           ...       ...       ...  ...       ...       ...       ...   \n",
       "words    0.167498 -0.055219 -0.034550  ...  0.191374  0.067224  0.094747   \n",
       "world    0.110620 -0.007374  0.079482  ...  0.174021  0.117663  0.186013   \n",
       "worth   -0.027766  0.021058  0.040095  ...  0.005939 -0.029942  0.072026   \n",
       "years    0.116394 -0.057514  0.108901  ...  0.123907  0.093873  0.154401   \n",
       "z        0.263103 -0.002866  0.113228  ...  0.321353  0.102310  0.393839   \n",
       "\n",
       "term         will   without     words     world     worth     years         z  \n",
       "term                                                                           \n",
       " [SEP]   0.053393  0.043304  0.035146 -0.019725  0.060074  0.073359  0.093997  \n",
       "##al     0.204611  0.234805  0.057567  0.339189  0.024444  0.080924  0.153247  \n",
       "##atic   0.094940  0.084769  0.002506  0.064155 -0.061443  0.042701  0.043055  \n",
       "##ation  0.106522  0.163783  0.150252  0.179236  0.088714  0.161013  0.495971  \n",
       "##ci     0.070276  0.153076  0.074848  0.114379  0.117168  0.045349  0.138723  \n",
       "...           ...       ...       ...       ...       ...       ...       ...  \n",
       "words    0.119390  0.149054  1.000000  0.108471  0.062540  0.099068  0.380922  \n",
       "world    0.149951  0.168116  0.108471  1.000000  0.027870  0.244823  0.253086  \n",
       "worth   -0.001627  0.058985  0.062540  0.027870  1.000000  0.051174  0.046957  \n",
       "years    0.097526  0.094451  0.099068  0.244823  0.051174  1.000000  0.212903  \n",
       "z        0.151623  0.276738  0.380922  0.253086  0.046957  0.212903  1.000000  \n",
       "\n",
       "[321 rows x 321 columns]"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_sim_matrix = cosine_similarity(df_outputs_embedding, df_outputs_embedding)\n",
    "\n",
    "cosine_sim_df = pd.DataFrame(cosine_sim_matrix, index=df_outputs_embedding.index, columns=df_outputs_embedding.index)\n",
    "\n",
    "pd.DataFrame(cosine_sim_matrix, index=df_outputs_embedding.index, columns=df_outputs_embedding.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058b2e40-7a4a-431f-8289-b316f115ef02",
   "metadata": {},
   "source": [
    "# Additional plans\n",
    "- Getting distance between words from two different documents\n",
    "- dimension reduction for plotting\n",
    "- use simple embedding like Word2Vec\n",
    "- same word across documents\n",
    "    similarity of words across different documents. should have high similarity. \n",
    "- Write up\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
