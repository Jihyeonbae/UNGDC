[
  {
    "objectID": "rmarkdown/liwc_randomforest.html",
    "href": "rmarkdown/liwc_randomforest.html",
    "title": "liwc_randomforest",
    "section": "",
    "text": "To compare the performance of logistic regression, we use non-parametric random forest models to see how well a battery of LIWC features can predict the regime type.\n\n\n\nlibrary(readr)\nlibrary(corrr)\nlibrary(ggplot2)\nlibrary(gmodels)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(countrycode)\nlibrary(broom)\nlibrary(knitr) \nlibrary(readxl)\nlibrary(rsample)\nlibrary(randomForest)\nlibrary(randomForestExplainer)\nlibrary(caret)\nlibrary(pROC)\nlibrary(kableExtra)",
    "crumbs": [
      "Linguistic Analysis",
      "LIWC Random Forest"
    ]
  },
  {
    "objectID": "rmarkdown/liwc_randomforest.html#data-and-setup",
    "href": "rmarkdown/liwc_randomforest.html#data-and-setup",
    "title": "liwc_randomforest",
    "section": "",
    "text": "library(readr)\nlibrary(corrr)\nlibrary(ggplot2)\nlibrary(gmodels)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(countrycode)\nlibrary(broom)\nlibrary(knitr) \nlibrary(readxl)\nlibrary(rsample)\nlibrary(randomForest)\nlibrary(randomForestExplainer)\nlibrary(caret)\nlibrary(pROC)\nlibrary(kableExtra)",
    "crumbs": [
      "Linguistic Analysis",
      "LIWC Random Forest"
    ]
  },
  {
    "objectID": "rmarkdown/liwc_randomforest.html#trial-1-including-wc-wps-period",
    "href": "rmarkdown/liwc_randomforest.html#trial-1-including-wc-wps-period",
    "title": "liwc_randomforest",
    "section": "Trial 1: Including WC, WPS, Period",
    "text": "Trial 1: Including WC, WPS, Period\nAs a baseline, the first trial keeps all of the LIWC features to the random forest model. This hit the accuracy rate of 0.7496. To make sure there is enough observations for both democracy and non-democracy, we use stratified sampling based on the main output of interest, “dd_democracy.” This means that the model performance can change when the distribution of the output is different from the current dataset. Our dataset has a well balanced distribution, with 4883 non-democracies and 4763 democracies. We randomly keep 30% of the data as a testing dataset.\nFunctionally, we use R packages (randomForestExplainer?; randomForest?) to generate various metrics of Importance.\n\nset.seed(3)\nsplit &lt;- rsample::initial_split(data, prop=0.7, strata=\"dd_democracy\")\ntrainN &lt;- rsample::training(split)\ntrainN$dd_democracy&lt;-factor(trainN$dd_democracy)\ntestN &lt;- rsample::testing(split)\n\n# Remove identifier variables. dd_regime is a 6-fold classification that is more comprehensive than dd_democracy.\ntrainN &lt;- trainN[, !(names(trainN) %in% c(\"ccode_iso\", \"year\", \"session\", \"dd_regime\"))]\ntestN &lt;- testN[, !(names(testN) %in% c(\"ccode_iso\", \"year\", \"session\", \"dd_regime\"))]\n\n# Remove rows with missing values\ntrainN &lt;- na.omit(trainN)\n\n# Fit random forest model\nbag.democracy1 &lt;- randomForest(dd_democracy ~ ., \n                               data = trainN, \n                               ntree = 500,\n                               mtry = ncol(trainN) - 1,\n                               importance = TRUE)\n\n# setting type to 1 selects Mean Accuracy Decrease, not Gini Coefficient. \nimportance(bag.democracy1,  type=1, scale = TRUE) %&gt;%\n  as.data.frame() %&gt;%\n  arrange(desc(abs(MeanDecreaseAccuracy))) %&gt;%head(20) %&gt;%\n  kbl() %&gt;%  kable_paper(\"hover\", full_width = FALSE) %&gt;%\n  scroll_box(width = \"100%\", height = \"400px\") \n\n\n\n\n\n\nMeanDecreaseAccuracy\n\n\n\n\nethnicity\n52.43919\n\n\nWPS\n40.94694\n\n\nPeriod\n38.65419\n\n\npower\n31.79705\n\n\nneed\n28.39407\n\n\ni\n28.20742\n\n\nmale\n24.63802\n\n\nwe\n20.74922\n\n\nprep\n20.62515\n\n\ndet\n20.46042\n\n\nverb\n20.29697\n\n\nfocusfuture\n19.64246\n\n\ntone_neg\n17.90792\n\n\nAuthentic\n17.84676\n\n\ntime\n17.73310\n\n\nemotion\n17.24758\n\n\nmoral\n17.14259\n\n\nauxverb\n17.06410\n\n\naffiliation\n16.23283\n\n\nDic\n15.76028\n\n\n\n\n\n\nvarImpPlot(bag.democracy1, type=1, scale = TRUE, sort=T, n.var= 25, main= \"Democracy vs. Non-democracy\", pch=16)\n\n\n\n\n\n\n\n# P-value on whether the observed number of successes &gt; the theoretical number of successes if random\nimportance_frame &lt;- measure_importance(bag.democracy1)\nimportance_frame%&gt;%\n  as.data.frame()%&gt;%\n  arrange(desc(abs(accuracy_decrease)))%&gt;% head(20) %&gt;%\n  kbl() %&gt;%  kable_paper(\"hover\", full_width = FALSE) %&gt;%\n  scroll_box(width = \"100%\", height = \"400px\") \n\n\n\n\n\nvariable\nmean_min_depth\nno_of_nodes\naccuracy_decrease\ngini_decrease\nno_of_trees\ntimes_a_root\np_value\n\n\n\n\nWPS\n1.33800\n4228\n0.0292639\n269.30627\n500\n344\n0.0000000\n\n\nPeriod\n3.13000\n3944\n0.0241194\n139.40684\n500\n156\n0.0000000\n\n\nethnicity\n1.19000\n3901\n0.0196658\n171.78294\n500\n0\n0.0000000\n\n\npower\n2.54200\n3936\n0.0110459\n93.92074\n500\n0\n0.0000000\n\n\nneed\n2.22800\n3657\n0.0085726\n71.88716\n500\n0\n0.0000000\n\n\nverb\n5.07824\n2162\n0.0072887\n37.01881\n494\n0\n0.9999992\n\n\nwe\n3.89208\n2725\n0.0065841\n48.25348\n498\n0\n0.0000000\n\n\ni\n3.35400\n4388\n0.0065604\n72.32486\n500\n0\n0.0000000\n\n\nmale\n2.80200\n3466\n0.0061633\n65.59172\n500\n0\n0.0000000\n\n\nauxverb\n6.49336\n2000\n0.0041989\n22.23185\n491\n0\n1.0000000\n\n\nprep\n5.44200\n3072\n0.0040095\n34.02075\n500\n0\n0.0000000\n\n\naffiliation\n5.89904\n2381\n0.0038055\n27.99589\n499\n0\n0.5835760\n\n\ntone_neg\n5.40904\n2658\n0.0036777\n30.89870\n499\n0\n0.0000000\n\n\ndet\n5.07000\n3109\n0.0036137\n37.64137\n500\n0\n0.0000000\n\n\nfocusfuture\n4.56200\n3573\n0.0035237\n46.24975\n500\n0\n0.0000000\n\n\nemotion\n4.28000\n2976\n0.0033963\n44.34134\n500\n0\n0.0000000\n\n\ndiscrep\n5.08304\n2693\n0.0029824\n36.01785\n499\n0\n0.0000000\n\n\nmoral\n4.21800\n3802\n0.0029608\n46.71324\n500\n0\n0.0000000\n\n\nAuthentic\n4.98208\n3068\n0.0028583\n37.35554\n498\n0\n0.0000000\n\n\ntime\n4.80400\n3647\n0.0028430\n42.62420\n500\n0\n0.0000000\n\n\n\n\n\n\n# scale = TRUE divides the permutation based measures into their \"standard errors\"\n#The function automatically scales the importance scores to be between 0 and 100. \n#Using scale = FALSE avoids this normalization step.\n\n\nTrial 1 Prediction Performance\n\nRT.pred1 &lt;- predict(bag.democracy1, newdata=testN, type=\"class\")\nRT.evlau1 &lt;- caret::confusionMatrix(as.factor(testN$dd_democracy), \n                                   RT.pred1, \n                                   positive = \"1\",\n                                   dnn = c(\"Reference\",\"Prediction\"))\nRT.evlau1\n\nConfusion Matrix and Statistics\n\n         Prediction\nReference    0    1\n        0 1081  384\n        1  313 1111\n                                          \n               Accuracy : 0.7587          \n                 95% CI : (0.7427, 0.7742)\n    No Information Rate : 0.5175          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.5177          \n                                          \n Mcnemar's Test P-Value : 0.008015        \n                                          \n            Sensitivity : 0.7431          \n            Specificity : 0.7755          \n         Pos Pred Value : 0.7802          \n         Neg Pred Value : 0.7379          \n             Prevalence : 0.5175          \n         Detection Rate : 0.3846          \n   Detection Prevalence : 0.4929          \n      Balanced Accuracy : 0.7593          \n                                          \n       'Positive' Class : 1               \n                                          \n\n# ROC curve and AUC\nRT.pred.roc1 &lt;- predict(bag.democracy1, newdata=testN, type=\"prob\")\nroc_RT.tree1 &lt;- roc(as.factor(testN$dd_democracy), RT.pred.roc1[,\"1\"])\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\npar(mfrow=c(1,1))\nplot(roc_RT.tree1, main=\"ROC curve for Random Forest\", \n     col=\"blue\", lwd=2, legacy.axes=FALSE)\ntitle(main = paste('Area under the curve: ',auc(roc_RT.tree1)))\n\n\n\n\n\n\n\n\n\n\nPost Analysis on Trial 1: Understanding WPS\nBased on several importance metrics, “WPS” showed up as important features. Given theoretically weak ties between WPS and regime type, we investigate sources of biases in the model. The specific question here is ?\n\nHypothesis 1: Correlated varaibles\nGenerally, random forest has a strong performance in even correlated variables. Nevertheless, we check if there are any features that are storngly correlated with WPS.\nWPS is correlated with the percentage of “preposition” words and “determiners.” These are grammatical terms that are expected to increase with longer sentences in general. “Period” is negatively correlated. In Trial 2, we exclude correlated features and test how robust the model is.\n\ntrainN %&gt;% \n    correlate() %&gt;% \n    focus(WPS) %&gt;% arrange(desc(abs(WPS))) %&gt;%head(20) %&gt;%\n  kbl() %&gt;%  kable_paper(\"hover\", full_width = FALSE) %&gt;%\n  scroll_box(width = \"100%\", height = \"400px\") \n\nNon-numeric variables removed from input: `text`, and `dd_democracy`\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\n\n\n\n\n\nterm\nWPS\n\n\n\n\nPeriod\n-0.6904750\n\n\nprep\n0.4428323\n\n\naffiliation\n-0.4018895\n\n\nAllPunc\n-0.3844678\n\n\ndet\n0.3819913\n\n\nClout\n-0.3778785\n\n\nWC\n0.3339915\n\n\nwe\n-0.3338740\n\n\narticle\n0.3317527\n\n\nipron\n0.3228187\n\n\nSocial\n-0.3142653\n\n\nDrives\n-0.3052480\n\n\ndiffer\n0.3039888\n\n\nfunction_feature\n0.2969884\n\n\nneed\n-0.2930493\n\n\nsocrefs\n-0.2837073\n\n\ntime\n-0.2719273\n\n\nrisk\n-0.2647073\n\n\nppron\n-0.2492513\n\n\nprosocial\n-0.2404551\n\n\n\n\n\n\n\n\n\nHypothesis 2: WPS has a strong predictive power\nIt might also be the case that WPS has a predictive power in a substantive manner. To make this null hypothesis as strong as possible, we only use a single feature, “WPS” to predict the output. Our expectation is a poor performance, in that we do not find any strong theoretical ground to expect a correlation between linguistic style to use a long sentence and a regime type.Contrary to our expectation, using WPS as a single predictor showed a great performance with the OOB estimate of error rate as low as 42.43%.\n\nset.seed(4)\nsplit &lt;- rsample::initial_split(data, prop=0.7, strata=\"dd_democracy\")\ntrainN &lt;- rsample::training(split)\ntrainN$dd_democracy&lt;-factor(trainN$dd_democracy)\ntestN &lt;- rsample::testing(split)\n\ntrainN &lt;- trainN[, (names(trainN) %in% c(\"WPS\", \"dd_democracy\"))]\ntestN &lt;- testN[, (names(testN) %in% c(\"WPS\", \"dd_democracy\"))]\n\n# Remove rows with missing values\ntrainN &lt;- na.omit(trainN)\n\n# Fit random forest model after removing missing values\nbag.democracy.h1 &lt;- randomForest(dd_democracy ~ ., \n                               data = trainN, \n                               ntree = 500,\n                               mtry = ncol(trainN) - 1,\n                               importance = TRUE)\nbag.democracy.h1\n\n\nCall:\n randomForest(formula = dd_democracy ~ ., data = trainN, ntree = 500,      mtry = ncol(trainN) - 1, importance = TRUE) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n        OOB estimate of  error rate: 42.43%\nConfusion matrix:\n     0    1 class.error\n0 2205 1220   0.3562044\n1 1649 1687   0.4943046\n\n\nWe further tried creating a binary variable of long or short sentences on average, based on the “WPS.” Using WPS’s median value as a threshold, we created a new variable “long,” 1 when the WPS is greater than or equal to 29 and 0 otherwise. Using it as the only predictor, we put the variable into a harder test. Even more surprisingly, the error rate turned out to be 35.18%.\n\nwps.vis&lt;-data%&gt;%\n  group_by(dd_democracy)%&gt;%\n  mutate(wps.median=median(WPS), \n         wps.sd=sd(WPS), \n         long=ifelse(WPS&gt;=29, 1, 0)\n         )\n\nset.seed(5)\nsplit &lt;- rsample::initial_split(wps.vis, prop=0.7, strata=\"dd_democracy\")\ntrainN &lt;- rsample::training(split)\ntrainN$dd_democracy&lt;-factor(trainN$dd_democracy)\ntestN &lt;- rsample::testing(split)\n\ntrainN &lt;- trainN[, (names(trainN) %in% c(\"long\", \"dd_democracy\"))]\ntestN &lt;- testN[, (names(testN) %in% c(\"long\", \"dd_democracy\"))]\ntrainN &lt;- na.omit(trainN)\n\nbag.democracy.h2 &lt;- randomForest(dd_democracy ~ long, \n                               data = trainN, \n                               ntree = 500,\n                               mtry = ncol(trainN) - 1,\n                               importance = TRUE)\nbag.RT.pred.h2 &lt;- predict(bag.democracy.h2, newdata = testN) \nRT.pred.h2 &lt;- predict(bag.democracy.h2, newdata=testN, type=\"class\")\nRT.evlau.h2 &lt;- caret::confusionMatrix(as.factor(testN$dd_democracy), \n                                   RT.pred.h2, \n                                   positive = \"1\",\n                                   dnn = c(\"Reference\",\"Prediction\"))\nRT.evlau.h2\n\nConfusion Matrix and Statistics\n\n         Prediction\nReference   0   1\n        0 940 531\n        1 517 918\n                                          \n               Accuracy : 0.6394          \n                 95% CI : (0.6216, 0.6568)\n    No Information Rate : 0.5014          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.2787          \n                                          \n Mcnemar's Test P-Value : 0.688           \n                                          \n            Sensitivity : 0.6335          \n            Specificity : 0.6452          \n         Pos Pred Value : 0.6397          \n         Neg Pred Value : 0.6390          \n             Prevalence : 0.4986          \n         Detection Rate : 0.3159          \n   Detection Prevalence : 0.4938          \n      Balanced Accuracy : 0.6394          \n                                          \n       'Positive' Class : 1               \n                                          \n\n\nIt requires more theoretical investigation to figure out why a linguistic tendency to use long sentences have a strong explanation for the regime type. We also suspect a confounding effect of whether the speech was translated.\n\n\nHypothesis 3: Language as a confounder\nIt can actually be the case that countries that are not speaking English lead to a confounding effect. Data available here. New variable “eng” is coded 1 when one of the spoken languages is English, 0 otherwise.\n\nlang&lt;-read_csv(\"data/raw/countries_languages.csv\")\n\nRows: 198 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Country, Languages Spoken\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# this is a metadata on country's official language\n\nlang&lt;-lang%&gt;%\n  mutate(ccode_iso = countrycode(Country, origin = 'country.name', destination = 'iso3c'))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `ccode_iso = countrycode(Country, origin = \"country.name\",\n  destination = \"iso3c\")`.\nCaused by warning:\n! Some values were not matched unambiguously: Kosovo, Micronesia\n\nlang&lt;-lang %&gt;%\n  mutate(eng=ifelse(str_detect(`Languages Spoken`, \"English\"), 1, 0))\n\nlang&lt;-lang%&gt;%\n  select(ccode_iso, eng)\n\nlang_data&lt;- left_join(data, lang, by=\"ccode_iso\")\n\nggplot(lang_data, aes(x=as.factor(eng), y=WPS)) + \n    geom_boxplot() \n\n\n\n\n\n\n\nset.seed(8)\nsplit &lt;- rsample::initial_split(data, prop=0.7, strata=\"dd_democracy\")\ntrainN &lt;- rsample::training(split)\ntrainN$dd_democracy&lt;-factor(trainN$dd_democracy)\ntestN &lt;- rsample::testing(split)\n\ntrainN &lt;- trainN[, (names(trainN) %in% c(\"WPS\", \"dd_democracy\"))]\ntestN &lt;- testN[, (names(testN) %in% c(\"WPS\", \"dd_democracy\"))]\n\n# Remove rows with missing values\ntrainN &lt;- na.omit(trainN)\n\n# Fit random forest model after removing missing values\nbag.democracy.h3 &lt;- randomForest(dd_democracy ~ ., \n                               data = trainN, \n                               ntree = 500,\n                               mtry = ncol(trainN) - 1,\n                               importance = TRUE)\nbag.democracy.h3\n\n\nCall:\n randomForest(formula = dd_democracy ~ ., data = trainN, ntree = 500,      mtry = ncol(trainN) - 1, importance = TRUE) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n        OOB estimate of  error rate: 44.03%\nConfusion matrix:\n     0    1 class.error\n0 2009 1418   0.4137730\n1 1567 1786   0.4673427",
    "crumbs": [
      "Linguistic Analysis",
      "LIWC Random Forest"
    ]
  },
  {
    "objectID": "rmarkdown/liwc_randomforest.html#trial-2-pca-on-liwc-features",
    "href": "rmarkdown/liwc_randomforest.html#trial-2-pca-on-liwc-features",
    "title": "liwc_randomforest",
    "section": "Trial 2: PCA on LIWC features",
    "text": "Trial 2: PCA on LIWC features\nTo account for high linearity among variables, including but not limited to WPS, we tried tried factor analysis to reduce the dimension. The results show that there are several vectors that represent the data without a strong skewness. The first representative vector explains only around 15% of the entire variance. Nevertheless, we fit the model with only the top five most representative components.\n\npca_data&lt;-data%&gt;%select(-text, -Segment, -ccode_iso, -dd_democracy)\npca_result&lt;- prcomp(pca_data,\n             center = TRUE,\n            scale. = TRUE)\n\nlibrary(factoextra)\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\nfviz_eig(pca_result)\n\n\n\n\n\n\n\nggplot(data = NULL) +\n  geom_segment(data = as.data.frame(pca_result$rotation), \n               aes(x = 0, y = 0, xend = PC1, yend = PC2),\n               arrow = arrow(length = unit(0.2, \"cm\")), color = 'blue') +\n  geom_text(data = as.data.frame(pca_result$rotation), \n            aes(label = rownames(pca_result$rotation), x = PC1, y = PC2), \n            size = 5, check_overlap = TRUE) +  # Adjust the size parameter here\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCross validation after PCA\nWe select the top 5 vectors that best represent the data. This will account for 30% of variances explained. After selecting 5 components, accuracy became 68%. We also used the top 3 components, which dropped the accuracy to 65%.\n\npca.df&lt;-pca_result$rotation\npca.df&lt;-pca.df%&gt;%as.data.frame()%&gt;%arrange(desc(abs(PC1)))\n\ncombined_data &lt;- cbind(pca_result$x[, 1:5], dd_democracy = data$dd_democracy)\ncombined_data &lt;- as.data.frame(combined_data)\n\ncombined_data$dd_democracy &lt;- as.factor(combined_data$dd_democracy)\n\nset.seed(6)\nsplit &lt;- rsample::initial_split(combined_data, prop = 0.7, strat = \"dd_democracy\")\ntrainN &lt;- rsample::training(split)\ntestN &lt;- rsample::testing(split)\n\n\ntrainN &lt;- trainN[, !(names(trainN) %in% c(\"ccode_iso\", \"year\", \"session\", \"dd_regime\"))]\ntestN &lt;- testN[, !(names(testN) %in% c(\"ccode_iso\", \"year\", \"session\", \"dd_regime\"))]\n\ntrainN &lt;- na.omit(trainN)\nbag.democracy.pca &lt;- randomForest(dd_democracy ~ ., \n                               data = trainN, \n                               ntree = 500,\n                               mtry = ncol(trainN) - 1,\n                               importance = TRUE)\n\n\npredictions &lt;- predict(bag.democracy.pca, newdata = testN)\nconfusionMatrix(data = predictions, reference = testN$dd_democracy)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    0    1\n         0 1009  446\n         1  462  976\n                                         \n               Accuracy : 0.6861         \n                 95% CI : (0.6689, 0.703)\n    No Information Rate : 0.5085         \n    P-Value [Acc &gt; NIR] : &lt;2e-16         \n                                         \n                  Kappa : 0.3722         \n                                         \n Mcnemar's Test P-Value : 0.6186         \n                                         \n            Sensitivity : 0.6859         \n            Specificity : 0.6864         \n         Pos Pred Value : 0.6935         \n         Neg Pred Value : 0.6787         \n             Prevalence : 0.5085         \n         Detection Rate : 0.3488         \n   Detection Prevalence : 0.5029         \n      Balanced Accuracy : 0.6861         \n                                         \n       'Positive' Class : 0              \n                                         \n\n\nTo understand the substantively important features, grouped by components, we checked importance metrics and the biplot.\n\nimportance(bag.democracy.pca,  type=1, scale = TRUE) %&gt;%\n  as.data.frame() %&gt;%\n  arrange(desc(abs(MeanDecreaseAccuracy))) %&gt;% head(20) %&gt;%\n  kbl() %&gt;%  kable_paper(\"hover\", full_width = FALSE) %&gt;%\n  scroll_box(width = \"100%\", height = \"400px\") \n\n\n\n\n\n\nMeanDecreaseAccuracy\n\n\n\n\nPC2\n135.05817\n\n\nPC3\n72.20042\n\n\nPC4\n47.44939\n\n\nPC1\n21.11336\n\n\nPC5\n18.64682\n\n\n\n\n\n\nggplot(data = NULL) +\n  geom_segment(data = as.data.frame(pca_result$rotation), \n               aes(x = 0, y = 0, xend = PC2, yend = PC3),\n               arrow = arrow(length = unit(0.2, \"cm\")), color = 'blue') +\n  geom_text(data = as.data.frame(pca_result$rotation), \n            aes(label = rownames(pca_result$rotation), x = PC2, y = PC3), \n            size = 5, check_overlap = TRUE) +  # Adjust the size parameter here\n  theme_minimal()+\n  labs(x = \"PC2\", y = \"PC3\")",
    "crumbs": [
      "Linguistic Analysis",
      "LIWC Random Forest"
    ]
  },
  {
    "objectID": "rmarkdown/liwc_randomforest.html#trial-3-excluding-wc-wps-and-period",
    "href": "rmarkdown/liwc_randomforest.html#trial-3-excluding-wc-wps-and-period",
    "title": "liwc_randomforest",
    "section": "Trial 3: excluding WC, WPS, and Period",
    "text": "Trial 3: excluding WC, WPS, and Period\nTo zero in on more meaningful features other than WPS, we left out some of the correlated features along with WPS. OOB estimate of error rate slightly increased around 0.5 percentage point(%p) after removing WC, WPS, and Period. Prediction accuracy against testN slightly dropped to 0.7409 accordingly.\n\nset.seed(7)\nsplit &lt;- rsample::initial_split(data, prop=0.7, strata=\"dd_democracy\")\ntrainN &lt;- rsample::training(split)\ntrainN$dd_democracy&lt;-factor(trainN$dd_democracy)\ntestN &lt;- rsample::testing(split)\n\ntrainN &lt;- trainN[, !(names(trainN) %in% c(\"ccode_iso\", \"year\", \"session\", \"dd_regime\", \"WC\", \"WPS\", \"Period\"))]\ntestN &lt;- testN[, !(names(testN) %in% c(\"ccode_iso\", \"year\", \"session\", \"dd_regime\", \"WC\", \"WPS\", \"Period\"))]\n\n# Remove rows with missing values\ntrainN &lt;- na.omit(trainN)\n\n# Fit random forest model after removing missing values\nbag.democracy3 &lt;- randomForest(dd_democracy ~ ., \n                               data = trainN, \n                               ntree = 500,\n                               mtry = ncol(trainN) - 1,\n                               importance = TRUE)\n\nvarImpPlot(bag.democracy3, sort=T, n.var= 25, main= \"Democracy vs. Non-democracy\", pch=16)\n\n\n\n\n\n\n\nRT.pred3 &lt;- predict(bag.democracy3, newdata=testN, type=\"class\")\n\nRT.evlau3 &lt;- caret::confusionMatrix(as.factor(testN$dd_democracy), \n                                   RT.pred3, \n                                   positive = \"1\",\n                                   dnn = c(\"Reference\",\"Prediction\"))\nRT.evlau3\n\nConfusion Matrix and Statistics\n\n         Prediction\nReference    0    1\n        0 1087  381\n        1  313 1115\n                                          \n               Accuracy : 0.7604          \n                 95% CI : (0.7444, 0.7758)\n    No Information Rate : 0.5166          \n    P-Value [Acc &gt; NIR] : &lt; 2e-16         \n                                          \n                  Kappa : 0.5209          \n                                          \n Mcnemar's Test P-Value : 0.01098         \n                                          \n            Sensitivity : 0.7453          \n            Specificity : 0.7764          \n         Pos Pred Value : 0.7808          \n         Neg Pred Value : 0.7405          \n             Prevalence : 0.5166          \n         Detection Rate : 0.3850          \n   Detection Prevalence : 0.4931          \n      Balanced Accuracy : 0.7609          \n                                          \n       'Positive' Class : 1               \n                                          \n\n# Importance matrix\nimportance(bag.democracy3, type=1, scale = TRUE) %&gt;%\n  as.data.frame() %&gt;%\n  arrange(desc(MeanDecreaseAccuracy)) %&gt;%head(20) %&gt;%\n  kbl() %&gt;%  kable_paper(\"hover\", full_width = FALSE) %&gt;%\n  scroll_box(width = \"100%\", height = \"400px\") \n\n\n\n\n\n\nMeanDecreaseAccuracy\n\n\n\n\nethnicity\n51.13591\n\n\npower\n33.46922\n\n\ni\n32.02684\n\n\nneed\n30.90368\n\n\nverb\n30.65794\n\n\nmale\n27.91114\n\n\nprep\n27.77320\n\n\ndet\n23.38925\n\n\nfocusfuture\n21.45934\n\n\nthey\n20.92268\n\n\ntime\n20.59665\n\n\naffiliation\n19.55701\n\n\narticle\n17.50262\n\n\nDic\n16.89642\n\n\nwe\n16.73911\n\n\npolite\n16.32449\n\n\nfemale\n16.15401\n\n\nPhysical\n16.04930\n\n\nauxverb\n16.04831\n\n\nemotion\n16.02271\n\n\n\n\n\n\n\nThere wasn’t a significant change in accuracy compared to Trial 1. We found one explanation from a chapter from Limitations of Interpretable Machine Learning Methods.\n\n\n\n\n\n\n“There is a drop in Leave-One-Covariate-Out(LOCO) Feature Importance of the two features the higher the correlation. However, in case of almost perfect multicollinearity, dropping the features that are correlated out of consideration to calculate the LOCO Feature Importance, the other feature can kind of”pick up” the effect on the target variable. As a consequence, there is no change in accuracy which means that there is only a small, up to no, increase in the error (parr2018?).”\n\n\n\n\nTrial 3.1) Subsetting aggregate-level features only\nWe suspected if the LIWC’s aggregate indices were correlated with their composite variables.\nAccuracy : 0.7132\n\nsubset&lt;-data%&gt;%\n  dplyr::select(Analytic, Clout, Authentic, Tone, BigWords, Linguistic, Dic, Drives,\n                Cognition, Affect, Culture, Lifestyle, Physical, Perception, Conversation,\n                AllPunc, dd_democracy)\n\nset.seed(4)\n\nsplit &lt;- rsample::initial_split(subset, prop=0.7, strata=\"dd_democracy\")\ntrainN &lt;- rsample::training(split)\ntrainN$dd_democracy&lt;-factor(trainN$dd_democracy)\ntestN &lt;- rsample::testing(split)\n\n# Remove rows with missing values\ntrainN &lt;- na.omit(trainN)\n\n# Fit random forest model after removing missing values\nbag.democracy3.1 &lt;- randomForest(dd_democracy ~ ., \n                               data = trainN, \n                               ntree = 500,\n                               mtry = ncol(trainN) - 1,\n                               importance = TRUE)\n\nvarImpPlot(bag.democracy3.1, sort=T, n.var= 15, main= \"Democracy vs. Non-democracy\", pch=16)\n\n\n\n\n\n\n\nbag.RT.pred3.1 &lt;- predict(bag.democracy3.1, newdata = testN) \n\nRT.pred3.1 &lt;- predict(bag.democracy3.1, newdata=testN, type=\"class\")\n\nRT.evlau3.1 &lt;- caret::confusionMatrix(as.factor(testN$dd_democracy), \n                                   RT.pred3.1, \n                                   positive = \"1\",\n                                   dnn = c(\"Reference\",\"Prediction\"))\nRT.evlau3.1\n\nConfusion Matrix and Statistics\n\n         Prediction\nReference   0   1\n        0 955 503\n        1 434 993\n                                          \n               Accuracy : 0.6752          \n                 95% CI : (0.6578, 0.6923)\n    No Information Rate : 0.5185          \n    P-Value [Acc &gt; NIR] : &lt; 2e-16         \n                                          \n                  Kappa : 0.3507          \n                                          \n Mcnemar's Test P-Value : 0.02632         \n                                          \n            Sensitivity : 0.6638          \n            Specificity : 0.6875          \n         Pos Pred Value : 0.6959          \n         Neg Pred Value : 0.6550          \n             Prevalence : 0.5185          \n         Detection Rate : 0.3442          \n   Detection Prevalence : 0.4946          \n      Balanced Accuracy : 0.6757          \n                                          \n       'Positive' Class : 1               \n                                          \n\n# Importance matrix\nimportance(bag.democracy3.1) %&gt;%\n  as.data.frame() %&gt;%\n  arrange(desc(abs(MeanDecreaseAccuracy))) %&gt;%head(20) %&gt;%\n  kbl() %&gt;%  kable_paper(\"hover\", full_width = FALSE) %&gt;%\n  scroll_box(width = \"100%\", height = \"400px\") \n\n\n\n\n\n\n0\n1\nMeanDecreaseAccuracy\nMeanDecreaseGini\n\n\n\n\nAnalytic\n40.865438\n40.5745666\n69.475826\n354.02036\n\n\nTone\n31.030152\n38.8144334\n49.865735\n294.86567\n\n\nDic\n14.952745\n37.0847678\n43.995731\n229.15114\n\n\nAffect\n28.986786\n15.3885397\n33.259720\n241.52373\n\n\nCulture\n27.213898\n16.0477721\n33.030883\n203.12302\n\n\nClout\n2.076986\n34.4868326\n33.008103\n226.76502\n\n\nAuthentic\n12.168968\n26.0004756\n31.905431\n214.44350\n\n\nLinguistic\n19.979601\n11.1680229\n29.746702\n185.47989\n\n\nBigWords\n14.040369\n16.4929244\n28.085091\n182.40524\n\n\nPhysical\n12.876732\n25.6306420\n27.995429\n222.85713\n\n\nCognition\n17.978734\n8.5972236\n25.942107\n185.94632\n\n\nAllPunc\n7.938512\n20.7900459\n21.466392\n212.65380\n\n\nDrives\n7.031147\n14.8450348\n19.890098\n174.94870\n\n\nLifestyle\n16.039245\n4.8331877\n17.043656\n204.61453\n\n\nPerception\n14.559775\n0.2686298\n11.574761\n181.72314\n\n\nConversation\n4.495709\n-0.6776919\n2.867841\n64.91052\n\n\n\n\n\n\n\n\n\nTrial 3.2) Subsetting lower-level features only\nWe also tried the opposite of Trial 3.1 by removing aggregate indices. There wasn’t a significant change in the overall accuracy.\nAccuracy : 0.7444\n\nsubset2&lt;-data[, !names(data) %in%\n                c(\"WC\", \"WPS\", \"BigWords\",\"Tone\", \"Analytic\", \"Clout\", \"Authentic\", \"Dic\", \"Linguistic\", \"Drives\", \"Cognition\", \"Affect\", \"Culture\", \"Lifestyle\", \"Physical\", \"Perception\", \"Conversation\", \"AllPunc\", \"function_features\", \"ppron\", \"emotion\", \"socbehav\", \"socrefs\", \"Culture\", \"Lifestyle\", \"Physical\", \"States\", \"Motives\", \"Perception\", \"Conversational\", \"Social\", \"Period\")]\n\nsubset2&lt;-subset2[, c(6:99)]\n\nset.seed(4)\n\nsplit &lt;- rsample::initial_split(subset2, prop=0.7, strata=\"dd_democracy\")\ntrainN &lt;- rsample::training(split)\ntrainN$dd_democracy&lt;-factor(trainN$dd_democracy)\ntestN &lt;- rsample::testing(split)\n\n# Remove rows with missing values\ntrainN &lt;- na.omit(trainN)\n\n# Fit random forest model after removing missing values\nbag.democracy3.2 &lt;- randomForest(dd_democracy ~ ., \n                               data = trainN, \n                               ntree = 500,\n                               mtry = ncol(trainN) - 1,\n                               importance = TRUE)\n\nvarImpPlot(bag.democracy3.2, sort=T, n.var= 25, main= \"Democracy vs. Non-democracy\", pch=16)\n\n\n\n\n\n\n\nbag.RT.pred3.2 &lt;- predict(bag.democracy3.2, newdata = testN) \n\nRT.pred3.2 &lt;- predict(bag.democracy3.2, newdata=testN, type=\"class\")\n\nRT.evlau3.2 &lt;- caret::confusionMatrix(as.factor(testN$dd_democracy), \n                                   RT.pred3.2, \n                                   positive = \"1\",\n                                   dnn = c(\"Reference\",\"Prediction\"))\nRT.evlau3.2\n\nConfusion Matrix and Statistics\n\n         Prediction\nReference    0    1\n        0 1076  382\n        1  353 1074\n                                          \n               Accuracy : 0.7452          \n                 95% CI : (0.7289, 0.7611)\n    No Information Rate : 0.5047          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.4905          \n                                          \n Mcnemar's Test P-Value : 0.3017          \n                                          \n            Sensitivity : 0.7376          \n            Specificity : 0.7530          \n         Pos Pred Value : 0.7526          \n         Neg Pred Value : 0.7380          \n             Prevalence : 0.5047          \n         Detection Rate : 0.3723          \n   Detection Prevalence : 0.4946          \n      Balanced Accuracy : 0.7453          \n                                          \n       'Positive' Class : 1               \n                                          \n\n# Importance matrix\nimportance(bag.democracy3.2) %&gt;%\n  as.data.frame() %&gt;%\n  arrange(desc(abs(MeanDecreaseAccuracy)))%&gt;%head(20) %&gt;%\n  kbl() %&gt;%  kable_paper(\"hover\", full_width = FALSE) %&gt;%\n  scroll_box(width = \"100%\", height = \"400px\") \n\n\n\n\n\n\n0\n1\nMeanDecreaseAccuracy\nMeanDecreaseGini\n\n\n\n\nethnicity\n40.496721\n31.074208\n50.09892\n221.37936\n\n\nneed\n15.734335\n33.871823\n36.28270\n124.83331\n\n\nverb\n31.315470\n13.138998\n34.59106\n97.08215\n\n\ni\n23.235183\n22.270993\n30.93661\n89.70045\n\n\nprep\n16.968264\n23.774738\n28.87460\n100.98803\n\n\nmale\n19.759043\n16.576991\n25.58376\n87.51931\n\n\npower\n18.891030\n17.563230\n25.32166\n87.31213\n\n\ntime\n20.056701\n14.606792\n24.44933\n72.75779\n\n\naffiliation\n19.047576\n10.563920\n24.26797\n55.20462\n\n\ndet\n18.228342\n11.364488\n21.72084\n64.99771\n\n\nfocusfuture\n13.306782\n15.162868\n20.25781\n57.26593\n\n\nwe\n13.938593\n17.412296\n19.91901\n89.74543\n\n\ntone_neg\n14.538008\n9.963408\n18.85690\n40.67337\n\n\npolite\n13.026583\n11.498368\n18.43403\n46.60988\n\n\nthey\n13.789092\n10.871015\n18.05035\n55.15415\n\n\nauxverb\n12.749960\n8.589462\n17.78425\n30.48857\n\n\nmoral\n17.453997\n7.680520\n17.67750\n56.75672\n\n\narticle\n9.150537\n11.787643\n16.66485\n45.86875\n\n\nfocuspresent\n6.011927\n13.206392\n16.21394\n30.35236\n\n\nallure\n10.644039\n8.607002\n15.72731\n31.95471\n\n\n\n\n\n\n\nRandom forest models did not show a better performance than logistic regression. We attribute this marginal performance to the fact that binary outcome is balanced. Previous studies have merited random forests to study rare events ((Muchlinski2016?), (Wang2019?)).",
    "crumbs": [
      "Linguistic Analysis",
      "LIWC Random Forest"
    ]
  },
  {
    "objectID": "rmarkdown/liwc_randomforest.html#trial-4-including-wc-wps-period-and-language-feature",
    "href": "rmarkdown/liwc_randomforest.html#trial-4-including-wc-wps-period-and-language-feature",
    "title": "liwc_randomforest",
    "section": "Trial 4 Including WC, WPS, Period and Language feature",
    "text": "Trial 4 Including WC, WPS, Period and Language feature\n\nset.seed(9)\nsplit &lt;- rsample::initial_split(lang_data, prop=0.7, strata=\"dd_democracy\")\ntrainN &lt;- rsample::training(split)\ntrainN$dd_democracy&lt;-factor(trainN$dd_democracy)\ntestN &lt;- rsample::testing(split)\n\n# Remove identifier variables. dd_regime is a 6-fold classification that is more comprehensive than dd_democracy.\ntrainN &lt;- trainN[, !(names(trainN) %in% c(\"ccode_iso\", \"year\", \"session\", \"dd_regime\"))]\ntestN &lt;- testN[, !(names(testN) %in% c(\"ccode_iso\", \"year\", \"session\", \"dd_regime\"))]\n\n# Remove rows with missing values\ntrainN &lt;- na.omit(trainN)\n\n# Fit random forest model\nbag.democracy4 &lt;- randomForest(dd_democracy ~ ., \n                               data = trainN, \n                               ntree = 500,\n                               mtry = ncol(trainN) - 1,\n                               importance = TRUE)\n\nbag.RT.pred4 &lt;- predict(bag.democracy4, newdata = testN) \nRT.pred4 &lt;- predict(bag.democracy4, newdata=testN, type=\"class\")\nRT.evlau4 &lt;- caret::confusionMatrix(as.factor(testN$dd_democracy), \n                                   RT.pred4, \n                                   positive = \"1\",\n                                   dnn = c(\"Reference\",\"Prediction\"))\nRT.evlau4\n\nConfusion Matrix and Statistics\n\n         Prediction\nReference    0    1\n        0 1112  350\n        1  337 1084\n                                          \n               Accuracy : 0.7617          \n                 95% CI : (0.7457, 0.7772)\n    No Information Rate : 0.5026          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.5234          \n                                          \n Mcnemar's Test P-Value : 0.6471          \n                                          \n            Sensitivity : 0.7559          \n            Specificity : 0.7674          \n         Pos Pred Value : 0.7628          \n         Neg Pred Value : 0.7606          \n             Prevalence : 0.4974          \n         Detection Rate : 0.3760          \n   Detection Prevalence : 0.4929          \n      Balanced Accuracy : 0.7617          \n                                          \n       'Positive' Class : 1               \n                                          \n\n# setting type to 1 selects Mean Accuracy Decrease, not Gini Coefficient. \nimportance(bag.democracy4,  type=1, scale = TRUE) %&gt;%\n  as.data.frame() %&gt;%\n  arrange(desc(abs(MeanDecreaseAccuracy))) %&gt;%head(20) %&gt;%\n  kbl() %&gt;%  kable_paper(\"hover\", full_width = FALSE) %&gt;%\n  scroll_box(width = \"100%\", height = \"400px\") \n\n\n\n\n\n\nMeanDecreaseAccuracy\n\n\n\n\nethnicity\n57.85337\n\n\nPeriod\n48.33889\n\n\nneed\n36.14936\n\n\ni\n31.42236\n\n\nWPS\n29.88887\n\n\npower\n28.01390\n\n\nmale\n27.68903\n\n\nthey\n23.98209\n\n\nmoral\n23.20073\n\n\nverb\n20.61054\n\n\nfocusfuture\n20.20344\n\n\nAuthentic\n18.01747\n\n\nmoney\n17.53209\n\n\nemotion\n17.42359\n\n\ntone_neg\n16.48298\n\n\ntime\n16.47235\n\n\ndiscrep\n16.24091\n\n\ndet\n15.86645\n\n\narticle\n15.28797\n\n\nAffect\n15.26143\n\n\n\n\n\n\nvarImpPlot(bag.democracy4, type=1, scale = TRUE, sort=T, n.var= 25, main= \"Democracy vs. Non-democracy\", pch=16)",
    "crumbs": [
      "Linguistic Analysis",
      "LIWC Random Forest"
    ]
  }
]